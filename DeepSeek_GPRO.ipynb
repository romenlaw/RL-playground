{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romenlaw/RL-playground/blob/main/DeepSeek_GPRO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[EVERY term in DeepSeek R1's GRPO explained (with examples and exercises) | RL Foundations](https://www.youtube.com/watch?v=mXWiDU9-fOk)"
      ],
      "metadata": {
        "id": "hcCThbaFiDgY"
      },
      "id": "hcCThbaFiDgY"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9c9135ee",
      "metadata": {
        "id": "9c9135ee"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantage\n",
        "It measures how well the answer has performed against the group.\n",
        "$$A_i=\\frac{r_i-mean({r_1, r_2, ... , r_G})}{std({r_1, r_2, ... , r_G})}$$\n",
        "\n",
        "The rewards (r) are 1 (for correct), 0 (for wrong).\n",
        "Wrong answers get negative advantage."
      ],
      "metadata": {
        "id": "212YMM2BfUF-"
      },
      "id": "212YMM2BfUF-"
    },
    {
      "cell_type": "code",
      "source": [
        "r = np.random.randint(0, 2, 10)\n",
        "m = r.mean()\n",
        "s = r.std()\n",
        "A = (r - m) / s\n",
        "r, m, s, A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZsa7MxGgp_E",
        "outputId": "6409eed1-bd62-4e91-8e5c-280063b5b743"
      },
      "id": "YZsa7MxGgp_E",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0]),\n",
              " np.float64(0.2),\n",
              " np.float64(0.4000000000000001),\n",
              " array([-0.5, -0.5, -0.5,  2. , -0.5, -0.5, -0.5,  2. , -0.5, -0.5]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy\n",
        "## current policy $\\pi_\\theta$\n",
        "Main terms of the objective formula:\n",
        "$\\pi_\\theta(o_i|q)A_i$\n",
        "\n",
        "* if $A_i>0$, i.e. the answer is correct, then adjust $\\theta$ (hence the policy) to increase the prob of $o_i$\n",
        "* if $A_i<0$, i.e. the answer is correct, then adjust $\\theta$ to decrease the prob of $o_i$\n",
        "\n",
        "## old policy $\\pi_{\\theta_{old}}$\n",
        "\n",
        "old version of the policy - e.g. policy of 10 gradient updates ago under an old version of LLM.\n",
        "\n",
        "The following term keeps the value between the 1-ϵ and 1+ϵ range, i.e. even if the new policy has increased a lot compared to the old, we don't want to update it beyond this range. i.e. be conservative about it's gradient updates. One intuition of this clipping is that even though the answer was wrong, it may be because one little step/part/token was wrong, so we don't want to over-penalise the model for the whole answer.\n",
        "$$clip\\left(\\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_old}(o_i|q)}, 1-\\epsilon, 1+\\epsilon\\right)A_i$$\n",
        "\n",
        "# the min term\n",
        "\n",
        "the formula takes the min of the clipped objective (times Adv) and the direct objective (times adv).\n",
        "\n",
        "It constrains how much to increase the prob of a particular question (q)'s response ($o_i$), and encourages the model\n",
        "1. to work on other reaponses of the question q ($o_1,... ,o_G$)\n",
        "2. to work on other questions' responses as well ($q\\sim Q$).\n",
        "\n"
      ],
      "metadata": {
        "id": "7cUa0zVwk5c1"
      },
      "id": "7cUa0zVwk5c1"
    },
    {
      "cell_type": "code",
      "source": [
        "def clip(input, low, high):\n",
        "  return np.minimum(np.maximum(input, low), high)"
      ],
      "metadata": {
        "id": "kmf40oAovd-a"
      },
      "id": "kmf40oAovd-a",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ratio is the pi/pi_old\n",
        "eps = 0.1\n",
        "\"\"\"\n",
        "scenarios:\n",
        "  A>0, ratio > 1+e\n",
        "  A>0, ratio < 1+e\n",
        "  A<0, ratio > 1-e\n",
        "  A<0, ratio < 1-e\n",
        "\"\"\"\n",
        "A=np.array([2, 2, -2, -2])\n",
        "ratio = np.array([1.2, 1, 1.2, 0.8])\n",
        "unclipped = ratio * A\n",
        "clipped = clip(ratio, 1-eps, 1+eps) * A"
      ],
      "metadata": {
        "id": "b_hnzAgpuxp8"
      },
      "id": "b_hnzAgpuxp8",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unclipped, clipped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki7yLj-KuxJf",
        "outputId": "7c0783d1-ee55-41bc-dc16-0cbfcb297ccb"
      },
      "id": "ki7yLj-KuxJf",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 2.4,  2. , -2.4, -1.6]), array([ 2.2,  2. , -2.2, -1.8]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.minimum(unclipped, clipped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxT25bBmwhab",
        "outputId": "5b8aa3c7-7859-4ea9-e063-3055783f0cf2"
      },
      "id": "wxT25bBmwhab",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.2,  2. , -2.4, -1.8])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The KL Divergence term\n",
        "\n",
        "It's KL divergence between the current policy vs. the reference policy (DeepSeek v3). It's another measure to keep the updates not too far from the reference/base model.\n",
        "\n",
        "If β is increased, the training will stay close to the base model (DeepSeek v3)."
      ],
      "metadata": {
        "id": "XZohkmGLx_v2"
      },
      "id": "XZohkmGLx_v2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}