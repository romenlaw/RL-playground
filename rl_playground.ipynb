{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romenlaw/RL-playground/blob/main/rl_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TXRzOQR4kYX",
        "outputId": "641e4f95-0b71-46bd-e1c4-13f8b9f6c85d"
      },
      "id": "8TXRzOQR4kYX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-walk\n",
            "  Cloning https://github.com/mimoralea/gym-walk to /tmp/pip-install-4crz1g2k/gym-walk_7189d96af84c406e976a99dcc3a1f8f7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mimoralea/gym-walk /tmp/pip-install-4crz1g2k/gym-walk_7189d96af84c406e976a99dcc3a1f8f7\n",
            "  Resolved https://github.com/mimoralea/gym-walk to commit 5999016267d6de2f5a63307fb00dfd63de319ac1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-walk) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (0.0.8)\n",
            "Building wheels for collected packages: gym-walk\n",
            "  Building wheel for gym-walk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-walk: filename=gym_walk-0.0.2-py3-none-any.whl size=4058 sha256=cf79ee378dcb8f6892108f4b67561055d812b6a7afcc71c3f46d8fd2b2f1e484\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mbvta0yw/wheels/24/fe/c4/0cbc7511d29265bad7e28a09311db3f87f0cafba74af54d530\n",
            "Successfully built gym-walk\n",
            "Installing collected packages: gym-walk\n",
            "Successfully installed gym-walk-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym, gym_walk\n",
        "import warnings\n",
        "\n",
        "# Ignore all DeprecationWarning\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "lPaUdewi3ncY"
      },
      "id": "lPaUdewi3ncY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MDP Concepts / Foundation\n",
        "Markov Decision Process:\n",
        "* Partially-Observable Markov Decision Process (POMDP): When the agent cannot fully observe the environment state.\n",
        "* Factored Markov Decision Process (FMDP): Allows the representation of the transition and reward function more compactly so that we can represent very large MDPs.\n",
        "* Continuous [Time|Action|State] Markov Decision Process: When either time, action, state or any combination of them are continuous.\n",
        "* Relational Markov Decision Process (RMDP): Allows the combination of probabilistic and relational knowledge.\n",
        "* Semi-Markov Decision Process (SMDP): Allows the inclusion of abstract actions that can take multiple time steps to complete.\n",
        "* Multi-Agent Markov Decision Process (MMDP): Allows the inclusion of multiple agents in the same environment.\n",
        "* Decentralized Markov Decision Process (Dec-MDP): Allows for multiple agents to collaborate and maximize a common reward.\n",
        "\n",
        "$MDP(\\mathcal S,\\mathcal A,\\mathcal T,\\mathcal R, \\mathcal S_{\\theta}, \\gamma, \\mathcal H)$\n",
        "\n",
        "$POMDP(\\mathcal S,\\mathcal A,\\mathcal T,\\mathcal R, \\mathcal S_{\\theta}, \\gamma, \\mathcal H, \\mathcal O, \\mathcal E)$\n",
        "\n",
        "$\\mathcal S$ is state space,\n",
        "$\\mathcal A$ is action space,\n",
        "$\\mathcal T$ is transition function,\n",
        "$\\mathcal R$ is reward signal,\n",
        "$\\mathcal S_{\\theta}$ is initial states distribution,\n",
        "$\\gamma$ is discount factor,\n",
        "$\\mathcal H$ is horizon,\n",
        "$\\mathcal O$ is observation space,\n",
        "$\\mathcal E$ is emission probability that defines the prob of showing an observation $o_t$ at given state $s_t$.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Tk0Fht_8BEY"
      },
      "id": "6Tk0Fht_8BEY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "* The outer dict keys are States.\n",
        "* The inner dict keys are Actions.\n",
        "\n",
        "{0: {0: [(0.8, 0, 0.0, True), ...], ...\n",
        "\n",
        "In State 0, Action 0 has 80% chance to transit into State 0, which has Reward 0.0 and is a terminal state..."
      ],
      "metadata": {
        "id": "TH17Y7MJ5p2W"
      },
      "id": "TH17Y7MJ5p2W"
    },
    {
      "cell_type": "code",
      "source": [
        "P = gym.make('BanditSlipperyWalk-v0').env.P\n",
        "print(f\"this environment has {len(P)} states\")\n",
        "print(f\"state 1 has transition rules: Action: [(prob, next_state, reward, next_state_is_terminal), (...), ...]: \\n\", P[1])\n",
        "P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfRXIV4K3vTG",
        "outputId": "ae887191-731c-460a-ebe0-da805aef5a5e"
      },
      "id": "hfRXIV4K3vTG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this environment has 3 states\n",
            "state 1 has transition rules: Action: [(prob, next_state, reward, next_state_is_terminal), (...), ...]: \n",
            " {0: [(0.8, 0, 0.0, True), (0.0, 1, 0.0, False), (0.2, 2, 1.0, True)], 1: [(0.8, 2, 1.0, True), (0.0, 1, 0.0, False), (0.2, 0, 0.0, True)]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)],\n",
              "  1: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)]},\n",
              " 1: {0: [(0.8, 0, 0.0, True), (0.0, 1, 0.0, False), (0.2, 2, 1.0, True)],\n",
              "  1: [(0.8, 2, 1.0, True), (0.0, 1, 0.0, False), (0.2, 0, 0.0, True)]},\n",
              " 2: {0: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)],\n",
              "  1: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P = gym.make('BanditWalk-v0').env.P\n",
        "P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlwp6NWa63-n",
        "outputId": "b9328f22-260c-470d-dfcb-fec596d48b91"
      },
      "id": "Vlwp6NWa63-n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)],\n",
              "  1: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)]},\n",
              " 1: {0: [(1.0, 0, 0.0, True), (0.0, 1, 0.0, False), (0.0, 2, 1.0, True)],\n",
              "  1: [(1.0, 2, 1.0, True), (0.0, 1, 0.0, False), (0.0, 0, 0.0, True)]},\n",
              " 2: {0: [(1.0, 2, 0.0, True), (0.0, 2, 0.0, True), (0.0, 2, 0.0, True)],\n",
              "  1: [(1.0, 2, 0.0, True), (0.0, 2, 0.0, True), (0.0, 2, 0.0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P = gym.make('FrozenLake-v1').env.P\n",
        "len(P),P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqbria-B7LMD",
        "outputId": "cd2e7433-e600-4270-ed1c-1174898655c1"
      },
      "id": "Zqbria-B7LMD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16,\n",
              " {0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
              "    (0.3333333333333333, 0, 0.0, False),\n",
              "    (0.3333333333333333, 4, 0.0, False)],\n",
              "   1: [(0.3333333333333333, 0, 0.0, False),\n",
              "    (0.3333333333333333, 4, 0.0, False),\n",
              "    (0.3333333333333333, 1, 0.0, False)],\n",
              "   2: [(0.3333333333333333, 4, 0.0, False),\n",
              "    (0.3333333333333333, 1, 0.0, False),\n",
              "    (0.3333333333333333, 0, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 1, 0.0, False),\n",
              "    (0.3333333333333333, 0, 0.0, False),\n",
              "    (0.3333333333333333, 0, 0.0, False)]},\n",
              "  1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
              "    (0.3333333333333333, 0, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, True)],\n",
              "   1: [(0.3333333333333333, 0, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, True),\n",
              "    (0.3333333333333333, 2, 0.0, False)],\n",
              "   2: [(0.3333333333333333, 5, 0.0, True),\n",
              "    (0.3333333333333333, 2, 0.0, False),\n",
              "    (0.3333333333333333, 1, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 2, 0.0, False),\n",
              "    (0.3333333333333333, 1, 0.0, False),\n",
              "    (0.3333333333333333, 0, 0.0, False)]},\n",
              "  2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
              "    (0.3333333333333333, 1, 0.0, False),\n",
              "    (0.3333333333333333, 6, 0.0, False)],\n",
              "   1: [(0.3333333333333333, 1, 0.0, False),\n",
              "    (0.3333333333333333, 6, 0.0, False),\n",
              "    (0.3333333333333333, 3, 0.0, False)],\n",
              "   2: [(0.3333333333333333, 6, 0.0, False),\n",
              "    (0.3333333333333333, 3, 0.0, False),\n",
              "    (0.3333333333333333, 2, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 3, 0.0, False),\n",
              "    (0.3333333333333333, 2, 0.0, False),\n",
              "    (0.3333333333333333, 1, 0.0, False)]},\n",
              "  3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
              "    (0.3333333333333333, 2, 0.0, False),\n",
              "    (0.3333333333333333, 7, 0.0, True)],\n",
              "   1: [(0.3333333333333333, 2, 0.0, False),\n",
              "    (0.3333333333333333, 7, 0.0, True),\n",
              "    (0.3333333333333333, 3, 0.0, False)],\n",
              "   2: [(0.3333333333333333, 7, 0.0, True),\n",
              "    (0.3333333333333333, 3, 0.0, False),\n",
              "    (0.3333333333333333, 3, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 3, 0.0, False),\n",
              "    (0.3333333333333333, 3, 0.0, False),\n",
              "    (0.3333333333333333, 2, 0.0, False)]},\n",
              "  4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
              "    (0.3333333333333333, 4, 0.0, False),\n",
              "    (0.3333333333333333, 8, 0.0, False)],\n",
              "   1: [(0.3333333333333333, 4, 0.0, False),\n",
              "    (0.3333333333333333, 8, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, True)],\n",
              "   2: [(0.3333333333333333, 8, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, True),\n",
              "    (0.3333333333333333, 0, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 5, 0.0, True),\n",
              "    (0.3333333333333333, 0, 0.0, False),\n",
              "    (0.3333333333333333, 4, 0.0, False)]},\n",
              "  5: {0: [(1.0, 5, 0, True)],\n",
              "   1: [(1.0, 5, 0, True)],\n",
              "   2: [(1.0, 5, 0, True)],\n",
              "   3: [(1.0, 5, 0, True)]},\n",
              "  6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, True),\n",
              "    (0.3333333333333333, 10, 0.0, False)],\n",
              "   1: [(0.3333333333333333, 5, 0.0, True),\n",
              "    (0.3333333333333333, 10, 0.0, False),\n",
              "    (0.3333333333333333, 7, 0.0, True)],\n",
              "   2: [(0.3333333333333333, 10, 0.0, False),\n",
              "    (0.3333333333333333, 7, 0.0, True),\n",
              "    (0.3333333333333333, 2, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 7, 0.0, True),\n",
              "    (0.3333333333333333, 2, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, True)]},\n",
              "  7: {0: [(1.0, 7, 0, True)],\n",
              "   1: [(1.0, 7, 0, True)],\n",
              "   2: [(1.0, 7, 0, True)],\n",
              "   3: [(1.0, 7, 0, True)]},\n",
              "  8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
              "    (0.3333333333333333, 8, 0.0, False),\n",
              "    (0.3333333333333333, 12, 0.0, True)],\n",
              "   1: [(0.3333333333333333, 8, 0.0, False),\n",
              "    (0.3333333333333333, 12, 0.0, True),\n",
              "    (0.3333333333333333, 9, 0.0, False)],\n",
              "   2: [(0.3333333333333333, 12, 0.0, True),\n",
              "    (0.3333333333333333, 9, 0.0, False),\n",
              "    (0.3333333333333333, 4, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 9, 0.0, False),\n",
              "    (0.3333333333333333, 4, 0.0, False),\n",
              "    (0.3333333333333333, 8, 0.0, False)]},\n",
              "  9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
              "    (0.3333333333333333, 8, 0.0, False),\n",
              "    (0.3333333333333333, 13, 0.0, False)],\n",
              "   1: [(0.3333333333333333, 8, 0.0, False),\n",
              "    (0.3333333333333333, 13, 0.0, False),\n",
              "    (0.3333333333333333, 10, 0.0, False)],\n",
              "   2: [(0.3333333333333333, 13, 0.0, False),\n",
              "    (0.3333333333333333, 10, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, True)],\n",
              "   3: [(0.3333333333333333, 10, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, True),\n",
              "    (0.3333333333333333, 8, 0.0, False)]},\n",
              "  10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
              "    (0.3333333333333333, 9, 0.0, False),\n",
              "    (0.3333333333333333, 14, 0.0, False)],\n",
              "   1: [(0.3333333333333333, 9, 0.0, False),\n",
              "    (0.3333333333333333, 14, 0.0, False),\n",
              "    (0.3333333333333333, 11, 0.0, True)],\n",
              "   2: [(0.3333333333333333, 14, 0.0, False),\n",
              "    (0.3333333333333333, 11, 0.0, True),\n",
              "    (0.3333333333333333, 6, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 11, 0.0, True),\n",
              "    (0.3333333333333333, 6, 0.0, False),\n",
              "    (0.3333333333333333, 9, 0.0, False)]},\n",
              "  11: {0: [(1.0, 11, 0, True)],\n",
              "   1: [(1.0, 11, 0, True)],\n",
              "   2: [(1.0, 11, 0, True)],\n",
              "   3: [(1.0, 11, 0, True)]},\n",
              "  12: {0: [(1.0, 12, 0, True)],\n",
              "   1: [(1.0, 12, 0, True)],\n",
              "   2: [(1.0, 12, 0, True)],\n",
              "   3: [(1.0, 12, 0, True)]},\n",
              "  13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
              "    (0.3333333333333333, 12, 0.0, True),\n",
              "    (0.3333333333333333, 13, 0.0, False)],\n",
              "   1: [(0.3333333333333333, 12, 0.0, True),\n",
              "    (0.3333333333333333, 13, 0.0, False),\n",
              "    (0.3333333333333333, 14, 0.0, False)],\n",
              "   2: [(0.3333333333333333, 13, 0.0, False),\n",
              "    (0.3333333333333333, 14, 0.0, False),\n",
              "    (0.3333333333333333, 9, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 14, 0.0, False),\n",
              "    (0.3333333333333333, 9, 0.0, False),\n",
              "    (0.3333333333333333, 12, 0.0, True)]},\n",
              "  14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
              "    (0.3333333333333333, 13, 0.0, False),\n",
              "    (0.3333333333333333, 14, 0.0, False)],\n",
              "   1: [(0.3333333333333333, 13, 0.0, False),\n",
              "    (0.3333333333333333, 14, 0.0, False),\n",
              "    (0.3333333333333333, 15, 1.0, True)],\n",
              "   2: [(0.3333333333333333, 14, 0.0, False),\n",
              "    (0.3333333333333333, 15, 1.0, True),\n",
              "    (0.3333333333333333, 10, 0.0, False)],\n",
              "   3: [(0.3333333333333333, 15, 1.0, True),\n",
              "    (0.3333333333333333, 10, 0.0, False),\n",
              "    (0.3333333333333333, 13, 0.0, False)]},\n",
              "  15: {0: [(1.0, 15, 0, True)],\n",
              "   1: [(1.0, 15, 0, True)],\n",
              "   2: [(1.0, 15, 0, True)],\n",
              "   3: [(1.0, 15, 0, True)]}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Return\n",
        "The Return is the sum of Rewards encountered from step t until the final step T:\n",
        "$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + ...+ R_T$$\n",
        "\n",
        "With discount factor:\n",
        "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + ...+ \\gamma^{T-1}R_T$$\n",
        "$$G_t = \\sum_{k=0}^\\infty \\gamma^kR_{t+1+k}$$\n",
        "\n",
        "In recursive form:\n",
        "$$G_t = R_{t+1} + \\gamma G_{t+1}$$"
      ],
      "metadata": {
        "id": "JZDDuPcQ__Qq"
      },
      "id": "JZDDuPcQ__Qq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State-Value Function V\n",
        "AKA Value Function, V-function, $V^\\pi(s)$...\n",
        "\n",
        "Value of state s when following a policy π:\n",
        "$$v_\\pi(s)=\\mathbb{E}_\\pi[G_t|S_t=s]$$\n",
        "Value of a State s under Policy \\pi is the Expectation over \\pi of Returns at time step t given you select State s at time step t.\n",
        "Use recursive form of Return:\n",
        "$$v_\\pi(s)=\\mathbb{E}_\\pi[R_t+\\gamma G_{t+1}|S_t=s]$$\n",
        "\n",
        "**Bellman equation**: it tells us how to find the value of states\n",
        "$$v_\\pi(s)=\\sum_a \\pi(a|s)\\sum_{s', r}\\{p(s',r|s,a)[r+\\gamma v_\\pi(s')]\\}, \\forall s \\in S$$\n",
        "\n",
        "* We get the action (or actions, if policy is stocastic) prescribed for state s. And do a weighted sum...\n",
        "* We also weight the sum over the probability of next states and rewards.\n",
        "* We add the reward and the discounted value of the landing states, weight that by the probabilities.\n",
        "* do this for all states in the state space\n",
        "\n"
      ],
      "metadata": {
        "id": "Dzu6DUvKFXOD"
      },
      "id": "Dzu6DUvKFXOD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action-Value Function Q\n",
        "What to expect from here if I do this?\n",
        "\n",
        "AKA Q-function. or $Q^\\pi(s,a)$: gives us the expected Return if agent follows policy π after taking action a in state s.\n",
        "\n",
        "$$q_\\pi(s, a)=\\mathbb{E}_\\pi[G_t|S_t=s, A_t=a]$$\n",
        "The value of Action a in State s is the expectation of Returns given we select action a in state s and follow policy π thereafter.\n",
        "\n",
        "In recursive form:\n",
        "$$q_\\pi(s,a) =  \\mathbb{E}_\\pi[R_t+\\gamma G_{t+1}|S_t=s, A_t=a]$$\n",
        "\n",
        "The Bellman equation for action-values:\n",
        "$$q_\\pi(s,a)=\\sum_{s',r}\\{p(s',r|s,a)[r+\\gamma v_\\pi(s')]\\}, \\forall s \\in S, \\forall a \\in A(s)$$\n"
      ],
      "metadata": {
        "id": "KTRTzoQ1K34A"
      },
      "id": "KTRTzoQ1K34A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action-Advantage Function\n",
        "i.e. how much better if I do this (comparing to taking the default action under policy π?\n",
        "\n",
        "The advantage of action a in state s under a policy π:\n",
        "$$a_\\pi(s,a)=q_\\pi(s,a)-v_\\pi(s)$$\n"
      ],
      "metadata": {
        "id": "NZ9O6sqWPiFz"
      },
      "id": "NZ9O6sqWPiFz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning\n",
        "Downside of Q-learning:\n",
        "* actions space is discrete and small\n",
        "* cannot handle continuous action spaces\n",
        "* policy is deterministically calculated from Q-function by maximising rewards -> cannot learn stochastic policies"
      ],
      "metadata": {
        "id": "MEMn8kMjnarI"
      },
      "id": "MEMn8kMjnarI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Evaluation -> V\n",
        "\n",
        "The policy evaluation equation\n",
        "$$v_{k+1}(s)=\\sum_a \\pi(a|s)\\sum_{s', a}p(s',r|s,a)[r+\\gamma v_k(s')]$$\n",
        "\n",
        "For a deterministic policy (e.g. we always move lefe), $\\sum_a \\pi(a|s)=1$"
      ],
      "metadata": {
        "id": "RbHQBFepvQ_1"
      },
      "id": "RbHQBFepvQ_1"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n",
        "  prev_V = np.zeros(len(P)) # initialise value for each state to 0\n",
        "  while True:\n",
        "    V = np.zeros(len(P))\n",
        "    for s in range(len(P)):\n",
        "      # example of P[s]pi[s]: [(0.8, 0, 0.0, True), (0.0, 1, 0.0, False), (0.2, 2, 1.0, True)]\n",
        "      for (prob, next_state, reward, done) in P[s][pi[s]]:\n",
        "        # multiply by not done s.t. V(s)=0 if s is terminal state\n",
        "        V[s] += prob * (reward + gamma * prev_V[next_state] * (not done) )\n",
        "    # stop the while loop if values have converged\n",
        "    if np.max(np.abs(prev_V - V)) < theta:\n",
        "      break\n",
        "\n",
        "    prev_V = V.copy()\n",
        "  return V"
      ],
      "metadata": {
        "id": "fMVvKZDq_-hL"
      },
      "id": "fMVvKZDq_-hL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate SWF\n",
        "Apply the above on Slippery Walk Five environment, using a policy of 'always left'.\n",
        "```\n",
        " _________________________________\n",
        "| H 0 | 1 | 2 | S 3 | 4 | 5 | G 6 |\n",
        " ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "```\n",
        "There are 7 states in this env, H=hole, G=Goal, S=Start"
      ],
      "metadata": {
        "id": "iuzh7Y9lnCZc"
      },
      "id": "iuzh7Y9lnCZc"
    },
    {
      "cell_type": "code",
      "source": [
        "P = gym.make('SlipperyWalkFive-v0').env.P\n",
        "len(P),P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTfYx4PUyXbx",
        "outputId": "d7c86692-297a-41f8-e3c6-136cded64d9d"
      },
      "id": "uTfYx4PUyXbx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7,\n",
              " {0: {0: [(0.5000000000000001, 0, 0.0, True),\n",
              "    (0.3333333333333333, 0, 0.0, True),\n",
              "    (0.16666666666666666, 0, 0.0, True)],\n",
              "   1: [(0.5000000000000001, 0, 0.0, True),\n",
              "    (0.3333333333333333, 0, 0.0, True),\n",
              "    (0.16666666666666666, 0, 0.0, True)]},\n",
              "  1: {0: [(0.5000000000000001, 0, 0.0, True),\n",
              "    (0.3333333333333333, 1, 0.0, False),\n",
              "    (0.16666666666666666, 2, 0.0, False)],\n",
              "   1: [(0.5000000000000001, 2, 0.0, False),\n",
              "    (0.3333333333333333, 1, 0.0, False),\n",
              "    (0.16666666666666666, 0, 0.0, True)]},\n",
              "  2: {0: [(0.5000000000000001, 1, 0.0, False),\n",
              "    (0.3333333333333333, 2, 0.0, False),\n",
              "    (0.16666666666666666, 3, 0.0, False)],\n",
              "   1: [(0.5000000000000001, 3, 0.0, False),\n",
              "    (0.3333333333333333, 2, 0.0, False),\n",
              "    (0.16666666666666666, 1, 0.0, False)]},\n",
              "  3: {0: [(0.5000000000000001, 2, 0.0, False),\n",
              "    (0.3333333333333333, 3, 0.0, False),\n",
              "    (0.16666666666666666, 4, 0.0, False)],\n",
              "   1: [(0.5000000000000001, 4, 0.0, False),\n",
              "    (0.3333333333333333, 3, 0.0, False),\n",
              "    (0.16666666666666666, 2, 0.0, False)]},\n",
              "  4: {0: [(0.5000000000000001, 3, 0.0, False),\n",
              "    (0.3333333333333333, 4, 0.0, False),\n",
              "    (0.16666666666666666, 5, 0.0, False)],\n",
              "   1: [(0.5000000000000001, 5, 0.0, False),\n",
              "    (0.3333333333333333, 4, 0.0, False),\n",
              "    (0.16666666666666666, 3, 0.0, False)]},\n",
              "  5: {0: [(0.5000000000000001, 4, 0.0, False),\n",
              "    (0.3333333333333333, 5, 0.0, False),\n",
              "    (0.16666666666666666, 6, 1.0, True)],\n",
              "   1: [(0.5000000000000001, 6, 1.0, True),\n",
              "    (0.3333333333333333, 5, 0.0, False),\n",
              "    (0.16666666666666666, 4, 0.0, False)]},\n",
              "  6: {0: [(0.5000000000000001, 6, 0.0, True),\n",
              "    (0.3333333333333333, 6, 0.0, True),\n",
              "    (0.16666666666666666, 6, 0.0, True)],\n",
              "   1: [(0.5000000000000001, 6, 0.0, True),\n",
              "    (0.3333333333333333, 6, 0.0, True),\n",
              "    (0.16666666666666666, 6, 0.0, True)]}})"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the always-left policy\n",
        "LEFT=0\n",
        "RIGHT=1\n",
        "n_states=len(P)\n",
        "pi_always_left={}\n",
        "for state in range(n_states): # ignore states 0 and 6 since they are terminal\n",
        "  pi_always_left[state] = LEFT"
      ],
      "metadata": {
        "id": "kui1Nl9NnTnW"
      },
      "id": "kui1Nl9NnTnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_always_left"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ta0tontQvz-",
        "outputId": "b23e8a9b-4f77-46f6-cd50-834641822576"
      },
      "id": "4ta0tontQvz-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V_pi_always_left = policy_evaluation(pi_always_left, P)"
      ],
      "metadata": {
        "id": "eDfC_4roRYm2"
      },
      "id": "eDfC_4roRYm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_pi_always_left"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZmVwtSGTAQt",
        "outputId": "da49e479-4bda-427a-eb24-00466a68c133"
      },
      "id": "kZmVwtSGTAQt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.00274725, 0.01098901, 0.03571429, 0.10989011,\n",
              "       0.33241758, 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate FL\n",
        "Frozen Lake: 16 states, 4 actions (L, D, R, U)\n",
        "We will evaluate 3 policies: random, eager, careful\n"
      ],
      "metadata": {
        "id": "k4cQcw5uV_De"
      },
      "id": "k4cQcw5uV_De"
    },
    {
      "cell_type": "code",
      "source": [
        "P = gym.make('FrozenLake-v1').env.P;\n",
        "n_states = len(P)\n",
        "# actions\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "decode_action = lambda i: ['L', 'D', 'R', 'U'][i]\n",
        "# I will use UP for holes since it's easier to type\n",
        "pi_random={\n",
        "    0:RIGHT, 1:LEFT, 2:DOWN, 3:UP,\n",
        "    4:LEFT, 5:UP, 6:RIGHT, 7:UP,\n",
        "    8:UP, 9:DOWN, 10:UP, 11:UP,\n",
        "    12:UP, 13:RIGHT, 14:DOWN, 15:UP\n",
        "    }"
      ],
      "metadata": {
        "id": "CJP97EM8XICu"
      },
      "id": "CJP97EM8XICu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_random=policy_evaluation(pi_random, P, 0.99) ;\n",
        "print(\"Random Policy value at START: %.4f\" % V_random[0]);\n",
        "V_random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCNcwjRcYdgs",
        "outputId": "9cdc0dea-4529-47ad-e6b1-8f91cfdcf251"
      },
      "id": "TCNcwjRcYdgs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Policy value at START: 0.0955\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09554433, 0.04705915, 0.0470064 , 0.04562386, 0.1469248 ,\n",
              "       0.        , 0.04976062, 0.        , 0.20275753, 0.26473443,\n",
              "       0.10378337, 0.        , 0.        , 0.49568466, 0.74165563,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "pi_true_random = {s:random.randint(0, 3) for s in range(n_states)}\n",
        "print(\"random policy: \", pi_true_random)\n",
        "V_true_random = policy_evaluation(pi_true_random, P, 0.99)\n",
        "print(\"True random Policy value at START: %.4f\" % V_true_random[0])\n",
        "V_true_random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ74fC69b78F",
        "outputId": "60762aa2-0ebb-4a2d-a758-b66d33b04e02"
      },
      "id": "rZ74fC69b78F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random policy:  {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 0, 6: 3, 7: 0, 8: 0, 9: 3, 10: 3, 11: 1, 12: 2, 13: 0, 14: 0, 15: 2}\n",
            "True random Policy value at START: 0.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_eager = {\n",
        "    0:RIGHT, 1:RIGHT, 2:DOWN, 3: LEFT,\n",
        "    4:DOWN, 5:UP, 6: DOWN, 7:UP,\n",
        "    8:RIGHT, 9:RIGHT, 10:DOWN, 11:UP,\n",
        "    12:UP, 13:RIGHT, 14:RIGHT, 15:UP\n",
        "}\n",
        "V_eager=policy_evaluation(pi_eager, P, 0.99) ;\n",
        "print(\"Eager Policy value at START: %.4f\" % V_eager[0]);\n",
        "V_eager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yowa-hhmYpnJ",
        "outputId": "e3ed4b8c-2da7-4b25-bfe1-aad3017dfe8c"
      },
      "id": "Yowa-hhmYpnJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eager Policy value at START: 0.0342\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03416037, 0.02305118, 0.04680089, 0.02305118, 0.04630471,\n",
              "       0.        , 0.09571851, 0.        , 0.0940126 , 0.23858195,\n",
              "       0.29005609, 0.        , 0.        , 0.43291953, 0.64037588,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_careful = {\n",
        "    0:LEFT, 1:UP, 2:UP, 3: UP,\n",
        "    4:LEFT, 5:UP, 6: UP, 7:UP,\n",
        "    8:UP, 9:DOWN, 10:LEFT, 11:UP,\n",
        "    12:UP, 13:RIGHT, 14:RIGHT, 15:UP\n",
        "}\n",
        "V_careful=policy_evaluation(pi_careful, P, 0.99) ;\n",
        "print(\"Careful Policy value at START: %.4f\" % V_careful[0]);\n",
        "V_careful"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfBJnVXQbNyw",
        "outputId": "7dfa115f-a59b-4c0e-dce8-669f9bf4ad43"
      },
      "id": "UfBJnVXQbNyw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Careful Policy value at START: 0.4079\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.4079433 , 0.3754127 , 0.35425824, 0.34383888, 0.42030522,\n",
              "       0.        , 0.11690522, 0.        , 0.44540366, 0.48399918,\n",
              "       0.43282831, 0.        , 0.        , 0.58843221, 0.71069653,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Improvement -> π\n",
        "To improve a policy, we use a state-value function and an MDP to get a one-step lookahead and determine which of the actions lead to the highest value. This is policy improvement equation:\n",
        "$$\\pi'(s)=arg\\max_{a}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\pi(s')]$$\n",
        "\n",
        "We obtain a new policy π' by taking the highest-valued action. How do we get the highest-valued action: by calculating for each action, the weighted sum of all rewards and values of all possible next states.\n",
        "\n",
        "Notice that this is simply using the action with the highest-valued Q-function.\n",
        "\n",
        "Downside of Q-learning:\n",
        "\n",
        "* actions space is discrete and small\n",
        "* cannot handle continuous action spaces\n",
        "* policy is deterministically calculated from Q-function by maximising rewards -> cannot learn stochastic policies\n",
        "\n"
      ],
      "metadata": {
        "id": "1gK9stMgbnDd"
      },
      "id": "1gK9stMgbnDd"
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax([1,2,2,1]), np.argmax([1,2,1,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9AsXff08201",
        "outputId": "d26e651d-2ec1-437d-e92c-9af2d37235c9"
      },
      "id": "y9AsXff08201",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(V, P, gamma=1.0):\n",
        "  n_states = len(P)\n",
        "  n_actions = len(P[0])\n",
        "  # initialise Q-function to zeros\n",
        "  Q = np.zeros((n_states, n_actions), dtype=np.float64)\n",
        "  for s in range(n_states):\n",
        "    for a in range(n_actions):\n",
        "      for prob, next_state, reward, done in P[s][a]:\n",
        "        Q[s,a] += prob * (reward + gamma * V[next_state] * (not done))\n",
        "  print(\"Q-function: \\n\", Q)\n",
        "  new_pi = {s:a for s, a in enumerate( np.argmax(Q, axis=1) )}\n",
        "  #new_pi = lambda s: {s:a for s, a in enumerate( np.argmax(Q, axis=1) )}[s]\n",
        "\n",
        "  return new_pi"
      ],
      "metadata": {
        "id": "sbOj6e8Gbqhu"
      },
      "id": "sbOj6e8Gbqhu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pi_careful = policy_improvement(V_careful, P, gamma=0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i9rvyufkLrr",
        "outputId": "d45c2395-df40-42dc-8a37-e4d8cb3cc98c"
      },
      "id": "1i9rvyufkLrr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-function: \n",
            " [[0.4079433  0.3972082  0.3972082  0.39312877]\n",
            " [0.25850748 0.25152651 0.24079141 0.3754127 ]\n",
            " [0.27937013 0.27593174 0.26895077 0.35425824]\n",
            " [0.23037205 0.23037205 0.22693366 0.34383888]\n",
            " [0.42030522 0.28568393 0.2816045  0.27332201]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.25973856 0.14283334 0.25973856 0.11690522]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.28568393 0.30670294 0.29842045 0.44540366]\n",
            " [0.34116584 0.48399918 0.33701597 0.28981655]\n",
            " [0.43282831 0.39424958 0.27310858 0.19829845]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.35390236 0.42871248 0.58843221 0.39424958]\n",
            " [0.57154583 0.76204582 0.71069653 0.6703493 ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_pi_careful)\n",
        "print({s:decode_action(new_pi_careful[s]) for s in range(n_states)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMsbLxg0oeGg",
        "outputId": "74816266-3026-4d02-bb68-2f674735a8c2"
      },
      "id": "dMsbLxg0oeGg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0, 1: 3, 2: 3, 3: 3, 4: 0, 5: 0, 6: 0, 7: 0, 8: 3, 9: 1, 10: 0, 11: 0, 12: 0, 13: 2, 14: 1, 15: 0}\n",
            "{0: 'L', 1: 'U', 2: 'U', 3: 'U', 4: 'L', 5: 'L', 6: 'L', 7: 'L', 8: 'U', 9: 'D', 10: 'L', 11: 'L', 12: 'L', 13: 'R', 14: 'D', 15: 'L'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d1={'a': 1, 'b': 2, 'c': 3}\n",
        "d2=d1.copy()\n",
        "#d2['c']=4\n",
        "d2==d1, d1==d2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYvLKUk9qMgT",
        "outputId": "915c9414-8285-455c-e5c6-7bc0cf4e47f5"
      },
      "id": "sYvLKUk9qMgT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration -> V, π*\n",
        "Policy iteration is guaranteed to converge to optimal policy. We need to make sure if there is a tie in action-value function, we need to have a way to deterministically choose an action (not randomly). The np.argmax() (used in policy_improvement) works deterministically - it uses the index of the first found element."
      ],
      "metadata": {
        "id": "F4ewcEyMwvxP"
      },
      "id": "F4ewcEyMwvxP"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def policy_iteration(V, P, gamma=1.0, theta=1e-10):\n",
        "  n_states = len(P)\n",
        "  n_actions = len(P[0])\n",
        "  # initialise policy to random actions.\n",
        "  pi = {s:random.randint(0, n_actions-1) for s in range(n_states)}\n",
        "\n",
        "  while True:\n",
        "    old_pi = pi.copy() # no nested dict, no shallow copy only\n",
        "    print(\"current Pi: \", [decode_action(a) for s, a in pi.items()])\n",
        "    V = policy_evaluation(pi, P, gamma, theta)\n",
        "    print(\"current V: \", V)\n",
        "    pi = policy_improvement(V, P, gamma)\n",
        "    if old_pi == pi:\n",
        "      break # stop improving if converged\n",
        "\n",
        "  return V, pi"
      ],
      "metadata": {
        "id": "oZtZdQ9txMZu"
      },
      "id": "oZtZdQ9txMZu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try it with a random V\n",
        "V, pi = policy_iteration(V_true_random, P, 0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u2RUP4Vz6B6",
        "outputId": "014ac941-52ce-4042-a5b7-ac16e79d52a5"
      },
      "id": "_u2RUP4Vz6B6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current Pi:  ['R', 'L', 'U', 'L', 'L', 'U', 'L', 'R', 'R', 'R', 'D', 'U', 'R', 'U', 'U', 'D']\n",
            "current V:  [0.02628938 0.0129485  0.00842033 0.00414733 0.04042691 0.\n",
            " 0.0670948  0.         0.0557895  0.12863218 0.19489724 0.\n",
            " 0.         0.19489724 0.46196551 0.        ]\n",
            "Q-function: \n",
            " [[0.03069187 0.02628938 0.02628938 0.021624  ]\n",
            " [0.0129485  0.01145421 0.00705172 0.01572721]\n",
            " [0.029193   0.02778291 0.02628861 0.00842033]\n",
            " [0.00414733 0.00414733 0.00273724 0.00551595]\n",
            " [0.04042691 0.03175142 0.02708603 0.02201638]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.0670948  0.06431609 0.0670948  0.00277871]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.03175142 0.06085915 0.0557895  0.07420003]\n",
            " [0.08272662 0.14704271 0.12863218 0.08272662]\n",
            " [0.21703852 0.19489724 0.1745899  0.0645899 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.10676471 0.21676471 0.25921332 0.19489724]\n",
            " [0.28108079 0.55009804 0.55009804 0.46196551]\n",
            " [0.         0.         0.         0.        ]]\n",
            "current Pi:  ['L', 'U', 'L', 'U', 'L', 'L', 'L', 'L', 'U', 'D', 'L', 'L', 'L', 'R', 'D', 'L']\n",
            "current V:  [0.53248009 0.4497886  0.38072706 0.36952921 0.54861585 0.\n",
            " 0.32320271 0.         0.58137634 0.63175429 0.59867508 0.\n",
            " 0.         0.73435551 0.85920993 0.        ]\n",
            "Q-function: \n",
            " [[0.53248009 0.5051919  0.5051919  0.4998671 ]\n",
            " [0.32414867 0.30135836 0.27407017 0.4497886 ]\n",
            " [0.38072706 0.37703177 0.35424146 0.39601481]\n",
            " [0.24758457 0.24758457 0.24388928 0.36952921]\n",
            " [0.54861585 0.37289742 0.36757262 0.35676166]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.32320271 0.19756278 0.32320271 0.12563993]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.37289742 0.40033311 0.38952215 0.58137634]\n",
            " [0.43419151 0.63175429 0.4399001  0.38941697]\n",
            " [0.59867508 0.49201819 0.39019617 0.31513581]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.45081623 0.52587659 0.73435551 0.49201819]\n",
            " [0.72343937 0.85920993 0.81443539 0.77323343]\n",
            " [0.         0.         0.         0.        ]]\n",
            "current Pi:  ['L', 'U', 'U', 'U', 'L', 'L', 'L', 'L', 'U', 'D', 'L', 'L', 'L', 'R', 'D', 'L']\n",
            "current V:  [0.54202593 0.49880319 0.47069569 0.4568517  0.55845096 0.\n",
            " 0.35834807 0.         0.59179874 0.64307982 0.61520756 0.\n",
            " 0.         0.74172044 0.86283743 0.        ]\n",
            "Q-function: \n",
            " [[0.54202593 0.52776242 0.52776242 0.52234217]\n",
            " [0.34347361 0.33419813 0.31993463 0.49880319]\n",
            " [0.43818949 0.43362097 0.4243455  0.47069569]\n",
            " [0.30609064 0.30609064 0.30152212 0.4568517 ]\n",
            " [0.55845096 0.3795824  0.37416214 0.36315737]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.35834807 0.20301849 0.35834807 0.15532958]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.3795824  0.40750993 0.39650516 0.59179874]\n",
            " [0.44006133 0.64307982 0.44778624 0.39831208]\n",
            " [0.61520756 0.49695269 0.40299122 0.33047121]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.45698409 0.5295041  0.74172044 0.49695269]\n",
            " [0.73252259 0.86283743 0.82108818 0.78111957]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(V)\n",
        "print([decode_action(a) for s, a in pi.items()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F4r0sOX0nci",
        "outputId": "0d3725d1-b5b8-4ce0-c6b0-f5bed8f988fd"
      },
      "id": "0F4r0sOX0nci",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.54202593 0.49880319 0.47069569 0.4568517  0.55845096 0.\n",
            " 0.35834807 0.         0.59179874 0.64307982 0.61520756 0.\n",
            " 0.         0.74172044 0.86283743 0.        ]\n",
            "['L', 'U', 'U', 'U', 'L', 'L', 'L', 'L', 'U', 'D', 'L', 'L', 'L', 'R', 'D', 'L']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration (VI) -> V, π*\n",
        "Improving behaviours early: only do one-step for policy evaluation, then do policy improvement.\n",
        "\n",
        "We can merge the truncated policy evaluation step and a policy improvement into the same equation:\n",
        "$$v_{k+1}(s) = \\max_a \\sum_{s', r}p(s',r|s,a)[r+\\gamma v_k(s')]$$\n",
        "\n",
        "Note that we don't deal with policies at all. It does it through value functions only, hence VI."
      ],
      "metadata": {
        "id": "rsmPVMwm9TyX"
      },
      "id": "rsmPVMwm9TyX"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def value_iteration(P, gamma=1.0, theta=1e-10):\n",
        "  n_states = len(P)\n",
        "  n_actions = len(P[0])\n",
        "  # initialise V to zeros\n",
        "  V = np.zeros(n_states, dtype=np.float64)\n",
        "\n",
        "  while True:\n",
        "    # initialise Q-function to zeros\n",
        "    Q = np.zeros((n_states, n_actions), dtype=np.float64)\n",
        "    for s in range(n_states):\n",
        "      for a in range(n_actions):\n",
        "        for prob, next_state, reward, done in P[s][a]:\n",
        "          Q[s,a] += prob * (reward + gamma * V[next_state] * (not done))\n",
        "    if np.max( np.abs(V - np.max(Q, axis=1)) ) < theta:\n",
        "      break; # if the action-advantage function converged, break\n",
        "\n",
        "    V = np.max(Q, axis=1) # combination of policy eval and improv\n",
        "  pi = {s:a for s, a in enumerate( np.argmax(Q, axis=1) )}\n",
        "\n",
        "  return V, pi\n"
      ],
      "metadata": {
        "id": "pxBuDHKM96a8"
      },
      "id": "pxBuDHKM96a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V2,pi2=value_iteration(P, 0.99)\n",
        "print(V2)\n",
        "print([decode_action(a) for s, a in pi2.items()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuENFpPVEUPU",
        "outputId": "4bee3f9f-9104-4a2e-b3d0-9d6bd9e9233a"
      },
      "id": "IuENFpPVEUPU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.54202593 0.49880318 0.47069569 0.4568517  0.55845096 0.\n",
            " 0.35834807 0.         0.59179874 0.64307982 0.61520756 0.\n",
            " 0.         0.74172044 0.86283743 0.        ]\n",
            "['L', 'U', 'U', 'U', 'L', 'L', 'L', 'L', 'U', 'D', 'L', 'L', 'L', 'R', 'D', 'L']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(V)\n",
        "print([decode_action(a) for s, a in pi.items()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh4ecdafFk1C",
        "outputId": "26c40992-c917-45b1-d330-093e278e5626"
      },
      "id": "Vh4ecdafFk1C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.54202593 0.49880319 0.47069569 0.4568517  0.55845096 0.\n",
            " 0.35834807 0.         0.59179874 0.64307982 0.61520756 0.\n",
            " 0.         0.74172044 0.86283743 0.        ]\n",
            "['L', 'U', 'U', 'U', 'L', 'L', 'L', 'L', 'U', 'D', 'L', 'L', 'L', 'R', 'D', 'L']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "pi==pi2, all(math.isclose(v1, v2, abs_tol=1e-10) for v1, v2 in zip(V, V2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCCPIlW7Fqma",
        "outputId": "e25772db-570f-424a-dbec-51cf67611395"
      },
      "id": "MCCPIlW7Fqma",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Armed Bandit (MAB)\n",
        "\n",
        "MABs are MDPs with\n",
        "* single non-terminal state, and\n",
        "* single timestep per episode (horizon=1)\n",
        "\n",
        "$$MAB = MDP(\\mathcal S=\\{s\\}, \\mathcal A, \\mathcal T, \\mathcal R, \\mathcal S_\\theta=\\{s\\}, \\gamma=1, \\mathcal H=1)$$\n",
        "\n",
        "Q-function of action a|s of MAB:\n",
        "$$q(a) = \\mathbb E [R_t|A_t=a]$$\n",
        "\n",
        "The best we can do in a MAB is represented by the optimal V-function, or selecting the action that maximises the Q-function:\n",
        "$$v^*=q(a_*)=\\max_{a \\in A}q(a)$$\n",
        "\n",
        "The optimal action is the action that maximises the optimal Q-function and optimal V-function (only 1 state)\n",
        "$$a_*=arg\\max_{a \\in A}q(a)$$"
      ],
      "metadata": {
        "id": "AAhQ3ZOqh_DD"
      },
      "id": "AAhQ3ZOqh_DD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total Regret\n",
        "\n",
        "The Total Regret equation:\n",
        "$$\\mathcal T = \\sum_{e=1}^E \\mathbb E[v_*-q_*(A_e)]$$\n",
        "To calculate the total regret: add up for all the episodes: the difference between the optimal value of the MAB and the *true value of the action selected*."
      ],
      "metadata": {
        "id": "WHt0dtDVrLgR"
      },
      "id": "WHt0dtDVrLgR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure Exploitation Strategy\n",
        "This baseline is called *greedy strategy*, or *pure exploitation strategy*. The greedy actionselection\n",
        "approach consists of always selecting the action with the highest estimated value.\n",
        "While there is a chance for the very first action, we choose to be the best overall action, the likelihood of this lucky coincidence decreases as the number of available actions increases."
      ],
      "metadata": {
        "id": "HE3gaexjzSVv"
      },
      "id": "HE3gaexjzSVv"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "def pure_exploitation(env, n_episodes=5000):\n",
        "  Q = np.zeros((env.action_space.n))\n",
        "  N = np.zeros((env.action_space.n))\n",
        "\n",
        "  # for monitoring and stats only, not necessary for the algorithm\n",
        "  Qe = np.empty((n_episodes, env.action_space.n))\n",
        "  returns = np.empty(n_episodes)\n",
        "  actions = np.empty(n_episodes)\n",
        "\n",
        "  name = 'Pure Exploitation'\n",
        "  for e in tqdm(range(n_episodes), desc=f\"Episodes for {name}\", leave=False):\n",
        "    action = np.argmax(Q)\n",
        "    _, reward, _, _ = env.step(action)\n",
        "    N[action] += 1\n",
        "    Q[action] = Q[action] + (reward - Q[action]) / N[action]\n",
        "\n",
        "    Qe[e] = Q\n",
        "    returns[e] = reward\n",
        "    actions[e] = action\n",
        "\n",
        "  return name, returns, Qe, actions"
      ],
      "metadata": {
        "id": "GE79-_F-iFCM"
      },
      "id": "GE79-_F-iFCM",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure Exploration Strategy\n",
        "This is another fundamental baseline which we can call a *random strategy* or a *pure exploration strategy*. This is simply an approach to action selection with no exploitation at all. The sole goal of the agent is to gain information."
      ],
      "metadata": {
        "id": "95Yq8BIU5jTx"
      },
      "id": "95Yq8BIU5jTx"
    },
    {
      "cell_type": "code",
      "source": [
        "def pure_exploration(env, n_episodes=5000):\n",
        "  Q = np.zeros((env.action_space.n))\n",
        "  N = np.zeros((env.action_space.n))\n",
        "\n",
        "  # for monitoring and stats only, not necessary for the algorithm\n",
        "  Qe = np.empty((n_episodes, env.action_space.n))\n",
        "  returns = np.empty(n_episodes)\n",
        "  actions = np.empty(n_episodes)\n",
        "\n",
        "  name = 'Pure Exploration'\n",
        "  for e in tqdm(range(n_episodes), desc=f\"Episodes for {name}\", leave=False):\n",
        "    action = np.random.randint(len(Q))\n",
        "    _, reward, _, _ = env.step(action)\n",
        "    N[action] += 1\n",
        "    Q[action] = Q[action] + (reward - Q[action]) / N[action]\n",
        "\n",
        "    Qe[e] = Q\n",
        "    returns[e] = reward\n",
        "    actions[e] = action\n",
        "\n",
        "  return name, returns, Qe, actions"
      ],
      "metadata": {
        "id": "_wqNfXaK126f"
      },
      "id": "_wqNfXaK126f",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ϵ-Greedy\n",
        "\n",
        "Almost always greedy, just sometimes random."
      ],
      "metadata": {
        "id": "0aJ7QHUV8EHE"
      },
      "id": "0aJ7QHUV8EHE"
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greed(env, epsilon=0.01, n_episodes=5000):\n",
        "  Q = np.zeros((env.action_space.n))\n",
        "  N = np.zeros((env.action_space.n))\n",
        "\n",
        "  # for monitoring and stats only, not necessary for the algorithm\n",
        "  Qe = np.empty((n_episodes, env.action_space.n))\n",
        "  returns = np.empty(n_episodes)\n",
        "  actions = np.empty(n_episodes)\n",
        "\n",
        "  name = f'Epsilon-Greedy {epsilon9}'\n",
        "  for e in tqdm(range(n_episodes), desc=f\"Episodes for {name}\", leave=False):\n",
        "    if np.random.random() > epsilon:\n",
        "      action = np.argmax(Q)\n",
        "    else:\n",
        "      action = np.random.randint(len(Q))\n",
        "\n",
        "    _, reward, _, _ = env.step(action)\n",
        "    N[action] += 1\n",
        "    Q[action] = Q[action] + (reward - Q[action]) / N[action]\n",
        "\n",
        "    Qe[e] = Q\n",
        "    returns[e] = reward\n",
        "    actions[e] = action\n",
        "\n",
        "  return name, returns, Qe, actions"
      ],
      "metadata": {
        "id": "ijt-51WU8PBb"
      },
      "id": "ijt-51WU8PBb",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decaying ϵ-Greedy\n",
        "First maximise exploration, then exploitation."
      ],
      "metadata": {
        "id": "SKK7DtVx9HKq"
      },
      "id": "SKK7DtVx9HKq"
    },
    {
      "cell_type": "code",
      "source": [
        "def lin_dec_epsilon_greedy(env,\n",
        "                           init_epsilon=1.0,\n",
        "                           min_epsilon=0.01,\n",
        "                           decay_ratio=0.05,\n",
        "                           n_episodes=5000):\n",
        ""
      ],
      "metadata": {
        "id": "WJ_ZvnJ19Qmr"
      },
      "id": "WJ_ZvnJ19Qmr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}