{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "CXH_dA7uqL9o"
      },
      "id": "CXH_dA7uqL9o"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk &>/dev/null\n",
        "!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima &>/dev/null"
      ],
      "metadata": {
        "id": "E26jsafbqPJA"
      },
      "id": "E26jsafbqPJA",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_walk, gym_aima\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from itertools import cycle, count\n",
        "import itertools\n",
        "from tabulate import tabulate\n",
        "\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "SEEDS = (12, 34, 56, 78, 90)\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('fivethirtyeight')\n",
        "params = {\n",
        "    'figure.figsize': (15, 8),\n",
        "    'font.size': 24,\n",
        "    'legend.fontsize': 20,\n",
        "    'axes.titlesize': 28,\n",
        "    'axes.labelsize': 24,\n",
        "    'xtick.labelsize': 20,\n",
        "    'ytick.labelsize': 20\n",
        "}\n",
        "pylab.rcParams.update(params)\n",
        "np.set_printoptions(suppress=True)"
      ],
      "metadata": {
        "id": "hDfePrOBqVOf"
      },
      "id": "hDfePrOBqVOf",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "ogaFETxXhCGO"
      },
      "id": "ogaFETxXhCGO"
    },
    {
      "cell_type": "code",
      "source": [
        "# slightly modified from chapter 4 - pi is lambda instead of dict\n",
        "def value_iteration(P, gamma=1.0, theta=1e-10):\n",
        "  n_states = len(P)\n",
        "  n_actions = len(P[0])\n",
        "  # initialise V to zeros\n",
        "  V = np.zeros(n_states, dtype=np.float64)\n",
        "\n",
        "  while True:\n",
        "    # initialise Q-function to zeros\n",
        "    Q = np.zeros((n_states, n_actions), dtype=np.float64)\n",
        "    for s in range(n_states):\n",
        "      for a in range(n_actions):\n",
        "        for prob, next_state, reward, done in P[s][a]:\n",
        "          Q[s,a] += prob * (reward + gamma * V[next_state] * (not done))\n",
        "    if np.max( np.abs(V - np.max(Q, axis=1)) ) < theta:\n",
        "      break; # if the action-advantage function converged, break\n",
        "\n",
        "    V = np.max(Q, axis=1) # combination of policy eval and improv\n",
        "  # pi = {s:a for s, a in enumerate( np.argmax(Q, axis=1) )}\n",
        "  pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
        "\n",
        "  return Q, V, pi\n",
        "\n",
        "def print_policy(pi, P, action_symbols=('<', 'v', '>', '^'), n_cols=4, title='Policy:'):\n",
        "  print(title)\n",
        "  arrs = {k:v for k,v in enumerate(action_symbols)}\n",
        "  for s in range(len(P)):\n",
        "    a = pi(s)\n",
        "    print(\"| \", end=\"\")\n",
        "    if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
        "        print(\"\".rjust(9), end=\" \")\n",
        "    else:\n",
        "        print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n",
        "    if (s + 1) % n_cols == 0: print(\"|\")\n",
        "\n",
        "def print_state_value_function(V, P, n_cols=4, prec=3, title='State-value function:'):\n",
        "  print(title)\n",
        "  for s in range(len(P)):\n",
        "    v = V[s]\n",
        "    print(\"| \", end=\"\")\n",
        "    if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
        "        print(\"\".rjust(9), end=\" \")\n",
        "    else:\n",
        "        print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n",
        "    if (s + 1) % n_cols == 0: print(\"|\")\n",
        "\n",
        "def print_action_value_function(Q,\n",
        "                                optimal_Q=None,\n",
        "                                action_symbols=('<', '>'),\n",
        "                                prec=3,\n",
        "                                title='Action-value function:'):\n",
        "  vf_types=('',) if optimal_Q is None else ('', '*', 'er')\n",
        "  headers = ['s',] + [' '.join(i) for i in list(itertools.product(vf_types, action_symbols))]\n",
        "  print(title)\n",
        "  states = np.arange(len(Q))[..., np.newaxis]\n",
        "  arr = np.hstack((states, np.round(Q, prec)))\n",
        "  if not (optimal_Q is None):\n",
        "    arr = np.hstack((arr, np.round(optimal_Q, prec), np.round(optimal_Q-Q, prec)))\n",
        "  print(tabulate(arr, headers, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "\"\"\"\n",
        "run the policy pi in the env for a number of episodes, collect and return\n",
        "results: % reached goal, avg reward, avg regret\n",
        "\"\"\"\n",
        "def get_policy_metrics(env, gamma, pi, goal_state, optimal_Q,\n",
        "                       n_episodes=100, max_steps=200):\n",
        "  random.seed(123); np.random.seed(123) ; env.seed(123)\n",
        "  reached_goal, episode_reward, episode_regret = [], [], []\n",
        "  for _ in range(n_episodes):\n",
        "    state, done, steps = env.reset(), False, 0\n",
        "    episode_reward.append(0.0)\n",
        "    episode_regret.append(0.0)\n",
        "    while not done and steps < max_steps:\n",
        "      action = pi(state)\n",
        "      regret = np.max(optimal_Q[state]) - optimal_Q[state][action]\n",
        "      episode_regret[-1] += regret\n",
        "\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      episode_reward[-1] += (gamma**steps * reward)\n",
        "\n",
        "      steps += 1\n",
        "\n",
        "    reached_goal.append(state == goal_state)\n",
        "  results = np.array((np.sum(reached_goal)/len(reached_goal)*100,\n",
        "                      np.mean(episode_reward),\n",
        "                      np.mean(episode_regret)))\n",
        "  return results\n",
        "\n",
        "def get_metrics_from_tracks(env, gamma, goal_state, optimal_Q, pi_track, coverage=0.1):\n",
        "  total_samples = len(pi_track)\n",
        "  n_samples = int(total_samples * coverage)\n",
        "  samples_e = np.linspace(0, total_samples, n_samples, endpoint=True, dtype=np.int)\n",
        "  metrics = []\n",
        "  for e, pi in enumerate(tqdm(pi_track)):\n",
        "    if e in samples_e:\n",
        "      metrics.append(get_policy_metrics(\n",
        "        env,\n",
        "        gamma=gamma,\n",
        "        pi=lambda s: pi[s],\n",
        "        goal_state=goal_state,\n",
        "        optimal_Q=optimal_Q))\n",
        "    else:\n",
        "      metrics.append(metrics[-1])\n",
        "  metrics = np.array(metrics)\n",
        "  success_rate_ma, mean_return_ma, mean_regret_ma = np.apply_along_axis(moving_average, axis=0, arr=metrics).T\n",
        "  return success_rate_ma, mean_return_ma, mean_regret_ma\n",
        "\n",
        "def mean_return(env, gamma, pi, n_episodes=100, max_steps=200):\n",
        "  random.seed(123); np.random.seed(123) ; env.seed(123)\n",
        "  results = []\n",
        "  for _ in range(n_episodes):\n",
        "    state, done, steps = env.reset(), False, 0\n",
        "    results.append(0.0)\n",
        "    while not done and steps < max_steps:\n",
        "      state, reward, done, _ = env.step(pi(state))\n",
        "      results[-1] += (gamma**steps * reward)\n",
        "      steps += 1\n",
        "  return np.mean(results)\n",
        "\n",
        "def rmse(x, y, dp=4):\n",
        "    return np.round(np.sqrt(np.mean((x - y)**2)), dp)\n",
        "\n",
        "\"\"\"exponentially decaying schedule\n",
        "this function allows you to calculate all the values for alpha for the full training process\n",
        "\"\"\"\n",
        "def decay_schedule(init_value, min_value,\n",
        "                   decay_ratio, # determines how many episodes to use for decay\n",
        "                   max_steps, # i.e. n_episodes in previous chapters\n",
        "                   log_start=-2, log_base=10):\n",
        "  assert min_value<=init_value, \"min_value must be <= init_value\"\n",
        "  decay_steps = max(int(max_steps*decay_ratio), 1)\n",
        "  rem_steps = max_steps - decay_steps # remaining steps (i.e. not used for decay)\n",
        "\n",
        "  # calculate actual values of an inverse log curve ([::-1] reverse the order)\n",
        "  values = np.logspace(start=log_start, stop=0,\n",
        "                       num=decay_steps, # number of samples to generate\n",
        "                       base=log_base,\n",
        "                       endpoint=True # samples are inclusive of 'stop'\n",
        "                       )[::-1]\n",
        "  # print(\"reverse logspace: \", values)\n",
        "  # normalise to between 0 and 1\n",
        "  values = (values - values.min()) / (values.max() - values.min())\n",
        "  # transform the points to lay between init_value and min_value\n",
        "  values = min_value + (init_value - min_value) * values\n",
        "  values = np.pad(values, (0, rem_steps), 'edge')\n",
        "  return values"
      ],
      "metadata": {
        "id": "xRnR0N2CuNX0"
      },
      "id": "xRnR0N2CuNX0",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting functions"
      ],
      "metadata": {
        "id": "18KLpLOSy1Tm"
      },
      "id": "18KLpLOSy1Tm"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_value_function(title, V_track, V_true=None, log=False, limit_value=0.05, limit_items=5):\n",
        "  np.random.seed(123)\n",
        "  per_col = 25\n",
        "  linecycler = cycle([\"-\",\"--\",\":\",\"-.\"])\n",
        "  legends = []\n",
        "\n",
        "  valid_values = np.argwhere(V_track[-1] > limit_value).squeeze()\n",
        "  items_idxs = np.random.choice(valid_values,\n",
        "                                min(len(valid_values), limit_items),\n",
        "                                replace=False)\n",
        "  # draw the true values first\n",
        "  if V_true is not None:\n",
        "    for i, state in enumerate(V_track.T):\n",
        "      if i not in items_idxs:\n",
        "        continue\n",
        "      if state[-1] < limit_value:\n",
        "        continue\n",
        "\n",
        "      label = 'v*({})'.format(i)\n",
        "      plt.axhline(y=V_true[i], color='k', linestyle='-', linewidth=1)\n",
        "      plt.text(int(len(V_track)*1.02), V_true[i]+.01, label)\n",
        "\n",
        "  # then the estimates\n",
        "  for i, state in enumerate(V_track.T):\n",
        "    if i not in items_idxs:\n",
        "      continue\n",
        "    if state[-1] < limit_value:\n",
        "      continue\n",
        "    line_type = next(linecycler)\n",
        "    label = 'V({})'.format(i)\n",
        "    p, = plt.plot(state, line_type, label=label, linewidth=3)\n",
        "    legends.append(p)\n",
        "\n",
        "  legends.reverse()\n",
        "\n",
        "  ls = []\n",
        "  for loc, idx in enumerate(range(0, len(legends), per_col)):\n",
        "    subset = legends[idx:idx+per_col]\n",
        "    l = plt.legend(subset, [p.get_label() for p in subset],\n",
        "                  loc='center right', bbox_to_anchor=(1.25, 0.5))\n",
        "    ls.append(l)\n",
        "  [plt.gca().add_artist(l) for l in ls[:-1]]\n",
        "  if log: plt.xscale('log')\n",
        "  plt.title(title)\n",
        "  plt.ylabel('State-value function')\n",
        "  plt.xlabel('Episodes (log scale)' if log else 'Episodes')\n",
        "  plt.show()\n",
        "\n",
        "def plot_transition_model(T_track, episode = 0):\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.view_init(elev=20, azim=50)\n",
        "\n",
        "  color_left = '#008fd5' # ax._get_lines.get_next_color()\n",
        "  color_right = '#fc4f30' #ax._get_lines.get_next_color()\n",
        "\n",
        "  left_prob = np.divide(T_track[episode][:,0].T,\n",
        "                        T_track[episode][:,0].sum(axis=1).T).T\n",
        "  left_prob = np.nan_to_num(left_prob, 0)\n",
        "\n",
        "  right_prob = np.divide(T_track[episode][:,1].T,\n",
        "                          T_track[episode][:,1].sum(axis=1).T).T\n",
        "  right_prob = np.nan_to_num(right_prob, 0)\n",
        "\n",
        "  for s in np.arange(9):\n",
        "    ax.bar3d(s+0.1, np.arange(9)+0.1, np.zeros(9),\n",
        "            np.zeros(9)+0.3,\n",
        "            np.zeros(9)+0.3,\n",
        "            left_prob[s],\n",
        "            color=color_left,\n",
        "            alpha=0.75,\n",
        "            shade=True)\n",
        "    ax.bar3d(s+0.1, np.arange(9)+0.1, left_prob[s],\n",
        "            np.zeros(9)+0.3,\n",
        "            np.zeros(9)+0.3,\n",
        "            right_prob[s],\n",
        "            color=color_right,\n",
        "            alpha=0.75,\n",
        "            shade=True)\n",
        "\n",
        "  ax.tick_params(axis='x', which='major', pad=10)\n",
        "  ax.tick_params(axis='y', which='major', pad=10)\n",
        "  ax.tick_params(axis='z', which='major', pad=10)\n",
        "\n",
        "  ax.xaxis.set_rotate_label(False)\n",
        "  ax.yaxis.set_rotate_label(False)\n",
        "  ax.zaxis.set_rotate_label(False)\n",
        "  ax.set_xticks(np.arange(9))\n",
        "  ax.set_yticks(np.arange(9))\n",
        "\n",
        "  plt.title('SWS learned MDP after {} episodes'.format(episode+1))\n",
        "  ax.set_xlabel('Initial\\nstate', labelpad=75, rotation=0)\n",
        "  ax.set_ylabel('Landing\\nstate', labelpad=75, rotation=0)\n",
        "  ax.set_zlabel('Transition\\nprobabilities', labelpad=75, rotation=0)\n",
        "\n",
        "  left_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_left)\n",
        "  right_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_right)\n",
        "\n",
        "  plt.legend((left_proxy, right_proxy),\n",
        "            ('Left', 'Right'),\n",
        "            bbox_to_anchor=(0.15, 0.9),\n",
        "            borderaxespad=0.)\n",
        "\n",
        "  ax.dist = 12\n",
        "  #plt.gcf().subplots_adjust(left=0.1, right=0.9)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_model_state_sampling(planning, algo='Dyna-Q'):\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "\n",
        "  color_left = '#008fd5' # ax._get_lines.get_next_color()\n",
        "  color_right = '#fc4f30' #ax._get_lines.get_next_color()\n",
        "\n",
        "  for s in np.arange(9):\n",
        "    actions = planning[np.where(planning[:,0]==s)[0], 1]\n",
        "    left = len(actions[actions == 0])\n",
        "    right = len(actions[actions == 1])\n",
        "    plt.bar(s, right, 0.2, color=color_right)\n",
        "    plt.bar(s, left, 0.2, color=color_left, bottom=right)\n",
        "\n",
        "\n",
        "  plt.title('States samples from {}\\nlearned model of SWS environment'.format(algo))\n",
        "  plt.xticks(range(9))\n",
        "  plt.xlabel('Initial states sampled', labelpad=20)\n",
        "  plt.ylabel('Count', labelpad=50, rotation=0)\n",
        "\n",
        "  left_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_left)\n",
        "  right_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_right)\n",
        "\n",
        "  plt.legend((left_proxy, right_proxy),\n",
        "            ('Left', 'Right'),\n",
        "            bbox_to_anchor=(0.99, 1.1),\n",
        "            borderaxespad=0.)\n",
        "\n",
        "  #plt.gcf().subplots_adjust(left=0.1, right=0.9)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_model_state_7(planning, algo='Dyna-Q'):\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "\n",
        "  color_left = '#008fd5' # ax._get_lines.get_next_color()\n",
        "  color_right = '#fc4f30' #ax._get_lines.get_next_color()\n",
        "\n",
        "\n",
        "  state_7 = planning[np.where(planning[:,0]==7)]\n",
        "  for sp in [6, 7, 8]:\n",
        "    actions = state_7[np.where(state_7[:,3]==sp)[0], 1]\n",
        "    left = len(actions[actions == 0])\n",
        "    right = len(actions[actions == 1])\n",
        "    plt.bar(sp, right, 0.2, color=color_right)\n",
        "    plt.bar(sp, left, 0.2, color=color_left, bottom=right)\n",
        "\n",
        "  plt.title('Next states samples by {}\\nin SWS environment from state 7'.format(algo))\n",
        "  plt.xticks([6,7,8])\n",
        "  plt.xlabel('Landing states', labelpad=20)\n",
        "  plt.ylabel('Count', labelpad=50, rotation=0)\n",
        "\n",
        "  left_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_left)\n",
        "  right_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_right)\n",
        "\n",
        "  plt.legend((left_proxy, right_proxy),\n",
        "            ('Left', 'Right'),\n",
        "            bbox_to_anchor=(0.99, 1.1),\n",
        "            borderaxespad=0.)\n",
        "\n",
        "  #plt.gcf().subplots_adjust(left=0.1, right=0.9)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "onZWB6lky35P"
      },
      "id": "onZWB6lky35P",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the replace-trace strategy, traces are set to 1 when a state-action pair is visited, and decay based on \\lambda value just like accumulate-trace strategy (in chapter 5).\n",
        "\n",
        "Diff between replace-trace and accumulate-trace:\n",
        "* accumulate-trace tracks the visited states; the eligibility trace is without bound\n",
        "* replace-trace tracks the visited state-action pair; also the eligibility trace is clipped to 1 to avoid dead loop"
      ],
      "metadata": {
        "id": "MdXxluwOc1KH"
      },
      "id": "MdXxluwOc1KH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Slippery Walk Seven"
      ],
      "metadata": {
        "id": "1aAb_N2u0Uf4"
      },
      "id": "1aAb_N2u0Uf4"
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('SlipperyWalkSeven-v0')\n",
        "init_state = env.reset()\n",
        "goal_state = 8\n",
        "gamma = 0.99\n",
        "n_episodes = 3000\n",
        "P = env.env.P\n",
        "n_cols, svf_prec, err_prec, avf_prec=9, 4, 2, 3\n",
        "action_symbols=('<', '>')\n",
        "limit_items, limit_value = 5, 0.0\n",
        "cu_limit_items, cu_limit_value, cu_episodes = 10, 0.0, 100\n",
        "\n",
        "optimal_Q, optimal_V, optimal_pi = value_iteration(P, gamma=gamma)\n",
        "print_state_value_function(optimal_V, P, n_cols=n_cols, prec=svf_prec, title='Optimal state-value function:')\n",
        "print()\n",
        "\n",
        "print_action_value_function(optimal_Q,\n",
        "                            None,\n",
        "                            action_symbols=action_symbols,\n",
        "                            prec=avf_prec,\n",
        "                            title='Optimal action-value function:')\n",
        "print()\n",
        "print_policy(optimal_pi, P, action_symbols=action_symbols, n_cols=n_cols)\n",
        "success_rate_op, mean_return_op, mean_regret_op = get_policy_metrics(\n",
        "    env, gamma=gamma, pi=optimal_pi, goal_state=goal_state, optimal_Q=optimal_Q)\n",
        "print('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n",
        "    success_rate_op, mean_return_op, mean_regret_op))"
      ],
      "metadata": {
        "id": "KRkQwGRm1Bqj",
        "outputId": "273be203-0383-41e9-f691-3fb2a7055ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KRkQwGRm1Bqj",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal state-value function:\n",
            "|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\n",
            "\n",
            "Optimal action-value function:\n",
            "╒═════╤═══════╤═══════╕\n",
            "│   s │     < │     > │\n",
            "╞═════╪═══════╪═══════╡\n",
            "│   0 │ 0     │ 0     │\n",
            "├─────┼───────┼───────┤\n",
            "│   1 │ 0.312 │ 0.564 │\n",
            "├─────┼───────┼───────┤\n",
            "│   2 │ 0.67  │ 0.763 │\n",
            "├─────┼───────┼───────┤\n",
            "│   3 │ 0.803 │ 0.845 │\n",
            "├─────┼───────┼───────┤\n",
            "│   4 │ 0.864 │ 0.889 │\n",
            "├─────┼───────┼───────┤\n",
            "│   5 │ 0.901 │ 0.922 │\n",
            "├─────┼───────┼───────┤\n",
            "│   6 │ 0.932 │ 0.952 │\n",
            "├─────┼───────┼───────┤\n",
            "│   7 │ 0.961 │ 0.981 │\n",
            "├─────┼───────┼───────┤\n",
            "│   8 │ 0     │ 0     │\n",
            "╘═════╧═══════╧═══════╛\n",
            "\n",
            "Policy:\n",
            "|           | 01      > | 02      > | 03      > | 04      > | 05      > | 06      > | 07      > |           |\n",
            "Reaches goal 96.00%. Obtains an average return of 0.8672. Regret of 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sarsa(λ)\n",
        "\n",
        "This is a mix of Sarsa and TD(λ) methods."
      ],
      "metadata": {
        "id": "RNtJEXXRe2-i"
      },
      "id": "RNtJEXXRe2-i"
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_lambda(env, gamma=1.0,\n",
        "                 init_alpha=0.5, min_alpha=0.01, alpha_decay_ratio=0.5,\n",
        "                 init_epsilon=1.0, min_epsilon=0.1, epsilon_decay_ratio=0.9,\n",
        "                 lambda_=0.5, # lambda is reserved word in python, so add _\n",
        "                 replacing_traces=True,\n",
        "                 n_episodes=3000):\n",
        "  nS = env.observation_sapece.n\n",
        "  nA = env.action_space.n\n",
        "  pi_track = []\n",
        "  Q = np.zeros((nS, nA), dtype=np.float32)\n",
        "  Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float32)\n",
        "  # eligibility trace keeps track of state-action pairs\n",
        "  E = np.zeros((nS, nA), dtype=np.float32)\n",
        "  select_action = lambda state, Q, epsilon: \\\n",
        "    np.argmax(Q[state]) if np.random.random() > epsilon \\\n",
        "    else np.random.randint(len(Q[state])) # random action of the state\n",
        "\n",
        "  alphas = decay_schedule(init_alpha, min_alpha, alphs_decay_ratio, n_episodes)\n",
        "  epsilons = decay_schedule(init_epsilon, min_epsilon, epsilon_decay_ratio, n_episodes)\n",
        "\n",
        "  for e in tqdm(range(n_episodes), leave=False):\n",
        "    E.fill(0) # at every episode, we reset eligitability of every state-action to 0\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    action = select_action(state, Q, epsilons[e])\n",
        "\n",
        "    while not done:\n",
        "      # collect experience\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      next_action = select_action(next_state, Q, epsilons[e])\n",
        "\n",
        "      # same as original sarsa\n",
        "      td_target = reward + gamma * Q[next_state][next_action] * (not done)\n",
        "      td_error = td_target - Q[state][action]\n",
        "\n",
        "      # increment state-action pair trace, clip to 1 if it's replacing trace\n",
        "      E[state][action]+=1\n",
        "      if replacing_traces: E.clip(0,1, out=E)\n",
        "\n",
        "      # notice we update entire Q table for all eligible state-action pairs\n",
        "      Q = Q + alphas[e] * td_error * E\n",
        "      E = gamma * lambda_ * E # decay E\n",
        "\n",
        "      state, action = next_state, next_action\n",
        "\n",
        "    Q_track[e] = Q\n",
        "    pi_track.append(np.argmax(Q[state], axis=1)) # axis 1 is action\n",
        "\n",
        "  V = np.max(Q, axis=1)\n",
        "  pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
        "\n",
        "  return Q, V, pi, Q_track, pi_track"
      ],
      "metadata": {
        "id": "uhfJ7WAdfF_b"
      },
      "id": "uhfJ7WAdfF_b",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Watkin's Q(λ)\n",
        "\n",
        "it's an off-policy control version of λ algorithms. Q(λ) is Q-learning using λ return for policy evaluation of the GPI pattern."
      ],
      "metadata": {
        "id": "hOSbj-wP_S_o"
      },
      "id": "hOSbj-wP_S_o"
    },
    {
      "cell_type": "code",
      "source": [
        "def q_lambda(env, gamma=1.0,\n",
        "             init_alpha=0.5, min_alpha=0.01, alpha_decay_ratio=0.5,\n",
        "             init_epsilon=1.0, min_epsilon=0.1, epsilon_decay_ratio=0.9,\n",
        "             lambda_=0.5,\n",
        "             replacing_traces=True,\n",
        "             n_episodes=3000):\n",
        "  nS = env.observation_space.n\n",
        "  nA = env.action_space.n\n",
        "  pi_track=[]\n",
        "  Q = np.zeros((nS, nA), dtype=np.float32)\n",
        "  Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float32)\n",
        "  E = np.zeros((nS, nA), dtype=np.float32)\n",
        "  select_action = lambda state, Q, epsilon: \\\n",
        "    np.argmax(Q[state]) if np.random.random() > epsilon \\\n",
        "    else np.random.randint(Q[state])\n",
        "  alphas = decay_schedule(init_alpha, min_alpha, alpha_decay_ratio, n_episodes)\n",
        "  epsilons = decay_schedule(init_epsilon, min_epsilon, epsilonn_decay_ratio, n_episodes)\n",
        "\n",
        "  for e in tqdm(range(n_episodes), leave=False):\n",
        "    E.fill(0)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    action = select_action(state, Q, epsilons[e])\n",
        "    while not done:\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      next_action = select_action(next_state, Q, epsilons[e])\n",
        "      # verify that action on next step is still from greedy policy\n",
        "      next_action_is_greedy = Q[next_state][next_action] == Q[next_state].max()\n",
        "      td_target = reward + gamma * Q[next_state].max() * (not done)\n",
        "      td_error = td_target - Q[state][action] # note: use current state\n",
        "\n",
        "      if replacing_traces: E[state].fill(0)\n",
        "\n",
        "      E[state][action] += 1\n",
        "      Q = Q + alphas[e] * td_error + E\n",
        "\n",
        "      if next_action_is_greedy:\n",
        "        E *= gamma * lambda_ # decay E as usual\n",
        "      else:\n",
        "        E.fill(0) # reset E because we want to learn greedy policy\n",
        "\n",
        "      state, action = next_state, next_action\n",
        "\n",
        "    Q_track[e] = Q\n",
        "    pi_track.append(np.max(Q, axis=1))\n",
        "\n",
        "  V = np.max(Q, axis=1)\n",
        "  pi = lambda s: {s:a for s,a in enumerate(np.argmax(Q, axis=1))}[s]\n",
        "  return Q"
      ],
      "metadata": {
        "id": "rfksOHiBA0mq"
      },
      "id": "rfksOHiBA0mq",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dyna-Q\n",
        "Unifying model-free and model-based/planning methods  by interleaving a model-free method (Q-learning) and a planning method (similar to Value Iteration)."
      ],
      "metadata": {
        "id": "SI-mQjIjEATy"
      },
      "id": "SI-mQjIjEATy"
    },
    {
      "cell_type": "code",
      "source": [
        "def dyna_q(env, gamma=1.0,\n",
        "           init_alpha=0.5, min_alpha=0.01, alpha_decay_ratio=0.5,\n",
        "           init_epsilon=1.0, min_epsilon=0.1, epsilon_decay_ratio=0.9,\n",
        "           n_planning=3, # number of updates to the estimates to run from learned model\n",
        "           n_episodes=3000\n",
        "           ):\n",
        "  nS = env.observation_space.n\n",
        "  nA = env.action_space.n\n",
        "  Q = np.zeros((nS, nA), dtype=np.float32)\n",
        "  Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float32)\n",
        "\n",
        "  # keep track of the transition function\n",
        "  T_count = np.zeros((nS, nA, nS), dtype=np.int16)\n",
        "  # keep track of reward signal\n",
        "  R_model = np.zeros((nS, nA, nS), dtype=np.float32)\n",
        "\n",
        "  select_action = lambda state, Q, epsilon: \\\n",
        "    np.argmax(Q[state]) if np.random.random()>epsilon \\\n",
        "    else np.random().randint(len(Q[state]))\n",
        "\n",
        "  alphas = decay_schedule(init_alpha, min_alpha, alpha_decay_ratio, n_episodes)\n",
        "  epsilons = decay_schedule(init_epsilon, min_epsilon, epsilon_decay_ratio, n_episodes)\n",
        "\n",
        "  for e in tqdm(range(n_episodes), leave=False):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = select_action(state, Q, epsilon[e])\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      # count number of full transitions, the (s, a, s') tuple\n",
        "      T_count[state][action][next_state] += 1\n",
        "\n",
        "      r_diff = reward - R_model[state][action][next_state]\n",
        "      R_model[state][action][next_state] += r_diff / T_count[state][action][next_state]\n",
        "\n",
        "      # same as Q-learning (off-policy, using the max)\n",
        "      td_target = reward + gamma * Q[next_state].max() * (not done)\n",
        "      td_error = td_target - Q[state][action]\n",
        "      Q[state][action] += alphas[e] * td_error\n"
      ],
      "metadata": {
        "id": "vWq0ytICIAaU"
      },
      "id": "vWq0ytICIAaU",
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}