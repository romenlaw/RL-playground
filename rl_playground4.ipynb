{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romenlaw/RL-playground/blob/main/rl_playground4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOSs-24eonWc"
      },
      "source": [
        "GPRL book chapter 8 - Introduction to Value-Based Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "iIKO9kDPq7Fb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "tvGhikWBonWd"
      },
      "outputs": [],
      "source": [
        "import warnings ; warnings.filterwarnings('ignore')\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "from itertools import cycle, count\n",
        "from textwrap import wrap\n",
        "\n",
        "import matplotlib\n",
        "import subprocess\n",
        "import os.path\n",
        "import tempfile\n",
        "import random\n",
        "import base64\n",
        "import pprint\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import sys\n",
        "import gym\n",
        "import io\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from gym import wrappers\n",
        "from subprocess import check_output\n",
        "from IPython.display import HTML\n",
        "\n",
        "LEAVE_PRINT_EVERY_N_SECS = 60\n",
        "ERASE_LINE = '\\x1b[2K'\n",
        "EPS = 1e-6\n",
        "BEEP = lambda: os.system(\"printf '\\a'\")\n",
        "RESULTS_DIR = os.path.join('..', 'results')\n",
        "SEEDS = (12, 34, 56, 78, 90)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('fivethirtyeight')\n",
        "params = {\n",
        "  'figure.figsize': (15, 8),\n",
        "  'font.size': 24,\n",
        "  'legend.fontsize': 20,\n",
        "  'axes.titlesize': 28,\n",
        "  'axes.labelsize': 24,\n",
        "  'xtick.labelsize': 20,\n",
        "  'ytick.labelsize': 20\n",
        "}\n",
        "pylab.rcParams.update(params)\n",
        "np.set_printoptions(suppress=True)"
      ],
      "metadata": {
        "id": "ESRKHpCmphAT"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxwFxqFGp0cy",
        "outputId": "a4b707fc-7611-4c2c-dce4-0befd4c0a9c6"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "aYnRQJUFq90a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_make_env_fn(**kargs):\n",
        "  def make_env_fn(env_name,\n",
        "                  seed=None,\n",
        "                  render=None,\n",
        "                  record=False,\n",
        "                  unwrapped=False,\n",
        "                  monitor_mode=None, # record video\n",
        "                  inner_wrappers=None,\n",
        "                  outer_wrappers=None):\n",
        "    tmp_dir = tempfile.mkdtemp()\n",
        "    env = None\n",
        "    if render:\n",
        "      try:\n",
        "        env = gym.make(env_name, render=render)\n",
        "      except:\n",
        "        pass\n",
        "    if env is None: env = gym.make(env_name)\n",
        "    if seed is not None: env.seed(seed)\n",
        "    env = env.unwrapped if unwrapped else env\n",
        "    if inner_wrappers:\n",
        "      for wrapper in inner_wrappers:\n",
        "        env = wrapper(env)\n",
        "\n",
        "    env = wrappers.Monitor(env, tmp_dir, force=True, # override temp file\n",
        "                           mode=monitor_mode,\n",
        "                           video_callable=lambda e_idx: record\n",
        "                           ) if monitor_mode else env\n",
        "    if outer_wrappers:\n",
        "      for wrapper in outer_wrappers:\n",
        "        env = wrapper(env)\n",
        "    return env\n",
        "  return make_env_fn, kargs"
      ],
      "metadata": {
        "id": "v-UdvyDQq_6C"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_videws_html(env_videos, title, max_n_videos=5):\n",
        "  '''\n",
        "  env_videos - list of videos\n",
        "  title - heading/title of the html heading\n",
        "  max_n_videos - max number of videos; can be larger or smaller tha len of the env_videos list\n",
        "  '''\n",
        "  videos = np.array(env_videos)\n",
        "  if len(videos) ==0:\n",
        "    return\n",
        "  n_videos = max(1, min(max_n_videos, len(videos)))\n",
        "  idxs = np.linspace(0, len(videos)-1, n_videos).astype(int) if n_videos>1 else [-1,]\n",
        "  videos = videos[idxs, ...]\n",
        "\n",
        "  strm = f'<h2>{title}</h2>'\n",
        "  for video_path, meta_path in videos:\n",
        "    video = io.open(video_path, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    with open(meta_path) as data_file:\n",
        "      meta = json.load(data_file)\n",
        "\n",
        "    html_tag=f\"\"\"\n",
        "    <h3>Episode {meta['episode_id']}</h3>\n",
        "    <video width=\"960\" height=\"540\" controls>\n",
        "      <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\"/>\n",
        "    </video>\n",
        "    \"\"\"\n",
        "    strm += html_tag\n",
        "  return strm\n"
      ],
      "metadata": {
        "id": "1x5fSIE8xN-d"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gif_html(env_videos, title, subtitle_eps=None, max_n_videos=4):\n",
        "  videos = np.array(env_videos)\n",
        "  if len(videos) ==0:\n",
        "    return\n",
        "  n_videos = max(1, min(max_n_videos, len(videos)))\n",
        "  idxs = np.linspace(0, len(videos)-1, n_videos).astype(int) if n_videos>1 else [-1,]\n",
        "  videos = videos[idxs, ...]\n",
        "\n",
        "  strm = f'<h2>{title}</h2>'\n",
        "  for video_path, meta_path in videos:\n",
        "    basename = os.path.splitext(video_path)[0]\n",
        "    gif_path = basename + '.gif'\n",
        "    if not os.path.exists(gif_path):\n",
        "      ps = subprocess.Popen((\n",
        "          'ffmpeg',\n",
        "          '-i', video_path,\n",
        "          '-r', '7',\n",
        "          '-f', 'image2pipe',\n",
        "          '-vcodec', 'ppm',\n",
        "          '-crf', '20',\n",
        "          '-vf', 'scale=512:-1',\n",
        "          '-'\n",
        "      ), stdout=subprocess.PIPE)\n",
        "      output = subprocess.check_output(\n",
        "          ('convert',\n",
        "           '-coalesce',\n",
        "           '-delay', '7',\n",
        "           '-loop', '0',\n",
        "           '-fuzz', '2%',\n",
        "           '+dither',\n",
        "           '-deconstruct',\n",
        "           '-layers', 'Optimize',\n",
        "           '-', gif_path),\n",
        "          stdin=ps.stdout)\n",
        "      ps.wait()\n",
        "\n",
        "    gif = io.open(gif_path, 'r+b').read()\n",
        "    encoded = base64.b64encode(gif)\n",
        "\n",
        "    with open(meta_path) as data_file:\n",
        "      meta = json.load(data_file)\n",
        "\n",
        "    prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n",
        "    sufix = meta['episode_id'] if subtitle_eps is None else subtitle_eps[meta['episode_id']]\n",
        "    html_tag = f\"\"\"\n",
        "    <h3>{prefix}{sufix}</h3>\n",
        "    <img src=\"data:image/gif;base64,{encoded.decode('ascii')}\"/>\n",
        "    \"\"\"\n",
        "    strm += html_tag\n",
        "  return strm"
      ],
      "metadata": {
        "id": "C_AxPsYkdgnn"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscountedCartPole(gym.Wrapper):\n",
        "  def __init__(self, env):\n",
        "    gym.Wrapper.__init__(self, env)\n",
        "\n",
        "  def reset(self, **kwargs):\n",
        "    return self.env.reset(**kwargs)\n",
        "\n",
        "  def setp(self, a):\n",
        "    # step returns observation, reward, terminated, truncated, info\n",
        "    o, r, d, _ = self.env.step(a)\n",
        "    (x, x_dot, theta, theta_dot) = o\n",
        "    pole_fell = x < -self.env.unwrapped.x_threshold \\\n",
        "      or x > self.env.unwrapped.x_threshold \\\n",
        "      or theta < -self.env.unwrapped.theta_threshold_radians \\\n",
        "      or theta > self.env.unwrapped.theta_threshold_radians\n",
        "    r = -1 if pole_fell else 0\n",
        "    return o, r, d, _"
      ],
      "metadata": {
        "id": "_5VrXG5IcsWR"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NFQ\n",
        "Neural Fitted Q Iteration (NFQ), one of the 1st neuro-net algo for reinforcment learning. It approximates action-value function - estimated as Q(s,a; θ).\n",
        "\n",
        "The architecture we use for cart pole is State-in-values-out:\n",
        "* input = state variables s: cart position, velocity, pole angle, velocity at tip\n",
        "* output = vector of values Q(s): one for each action"
      ],
      "metadata": {
        "id": "bZD5PfjfeaK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fully connected Q-function (state-in-values-out)\n",
        "class FCQ(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dims=(32,32), activation_fc=F.relu):\n",
        "    super(FCQ, self).__init__()\n",
        "    self.activation_fc = activation_fc\n",
        "\n",
        "    self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
        "    self.hidden_layers = nn.ModuleList()\n",
        "    for i in range(len(hidden_dims)-1):\n",
        "      hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
        "      self.hidden_layers.append(hidden_layer)\n",
        "\n",
        "    self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
        "\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.device = torch.device(device)\n",
        "    self.to(self.device)\n",
        "\n",
        "  def _format(self, state):\n",
        "    \"\"\"\n",
        "    state: is a list of state variables, cart position, velocity, etc.\n",
        "    \"\"\"\n",
        "    x = state\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "      x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
        "      x = x.unsqueeze(0)\n",
        "    return x\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self._format(state)\n",
        "    x = self.activation_fc(self.input_layer(x))\n",
        "    for hidden_layer in self.hidden_layers:\n",
        "      x = self.activation_fc(hidden_layer(x))\n",
        "    x = self.output_layer(x)\n",
        "    return x\n",
        "\n",
        "  def numpy_float_to_device(self, variable):\n",
        "    variable = torch.from_numpy(variable).float().to(self.device)\n",
        "    return variable\n",
        "\n",
        "  def load(self, experiences):\n",
        "    states, actions, rewards, new_states, is_terminals = experiences\n",
        "    states = torch.from_numpy(states).float().to(self.device)\n",
        "    actions = torch.from_numpy(actions).long().to(self.device)\n",
        "    new_states = torch.from_numpy(new_states).float().to(self.device)\n",
        "    rewards = torch.from_numpy(rewards).float().to(self.device)\n",
        "    is_termnials = torch.from_numpy(is_terminals).float().to(self.device)\n",
        "    return states, actions, rewards, new_states, is_terminals\n"
      ],
      "metadata": {
        "id": "RDscVGqDeb6V"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## selecting an exploration strategy\n",
        "\n",
        "Any techniques from chapter 4 for exploration-exploitation tradeoff will be fine. To keep it simple, we use an epsilon-greedy strategy on our NFQ implementation.\n",
        "\n",
        "But we are trining an off-policy learning algo, i.e. there are two policies:\n",
        "* a policy that generates behaviour, which in this case is an ϵ-greedy policy\n",
        "* a policy that we are learning about, which is the greedy (an ultimately optimal) policy."
      ],
      "metadata": {
        "id": "QkqjMKdpnLbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedyStrategy():\n",
        "  def __init__(self):\n",
        "    self.exploratory_action_taken = False\n",
        "\n",
        "  def select_action(self, model, state):\n",
        "    with torch.no_grad():\n",
        "      q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
        "      return np.argmax(q_values)"
      ],
      "metadata": {
        "id": "5Z6mauKHVq1r"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EGreedyStrategy():\n",
        "  def __init__(self, epsilon=0.1):\n",
        "    self.epsilon = epsilon\n",
        "    self.exploratory_action_taken = None\n",
        "\n",
        "  def select_action(self, model, state):\n",
        "    self.exploratory_action_taken = False\n",
        "    with torch.no_grad():\n",
        "      q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
        "\n",
        "    if np.random.rand() > self.epsilon:\n",
        "      action = np.argmax(q_values)\n",
        "    else:\n",
        "      action = np.random.randint(len(q_values))\n",
        "\n",
        "    self.exploratory_action_taken = action != np.argmax(q_values)\n",
        "    return action"
      ],
      "metadata": {
        "id": "EiAU_HaeWH-T"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## selecting what to optimise\n",
        "An ideal objective in value-based DRL would be to minimise the loss w.r.t. the optimal action-value function q⋆.\n",
        "$$L_i(θ_i)=\\mathbb{E}_{s,a}\\left[\\left(q_*(s,a)-Q(s,a;\\theta_i)\\right)^2\\right]\n",
        "$$\n",
        "Because we'd like to have an estimate of q*, Q, that tracks exactly that optimal function, if we had a solid estimate of q*, we then could use a greedy action w.r.t. these estimates to get near optimal behaviour.\n",
        "\n",
        "Refesh the definition of optimal action=value function q⋆:\n",
        "$$q_*(s,a)=\\max_{π}\\mathbb{E}_π\\left[G_t|S_t=s, A_t=a\\right], ∀s∈S,∀a\\in A(s)\n",
        "$$\n",
        "The optimal action-value function q⋆ is the poicy π that gives the maximum expected return from each and every action in each and every state.\n",
        "\n",
        "For our NFQ implementation, we start with a randomly initialised q function (and implicit policy). Then evaluate the policy by sampling actions from it, same as chapter 5. Then iprove it with an exploration strategy such as epsilon-greedy, like in chapter 4. Finally, keep iterating until we reach the desired peformance, as in chapters 6 and 7."
      ],
      "metadata": {
        "id": "O1EkTM4OuIoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## selecting targets for policy evaluation\n",
        "Out of MC, TD, N-step, Lambda targets, any one would do. For simplicity we choose TD target.\n",
        "\n",
        "TD targets can be either\n",
        "* on-policy: Sarsa target for on-policy bootstrapping - use the action-value function of the action the agent will take at the landing state; the target would be approximatin gthe behavioural policy, i.e. the policy-generating behaviour and the policy being learned would be the same.\n",
        "* off-policy: Q-learning for off-policy bootstrapping - use the value of the best action at the next state; we always approximate the greedy poicy, even if the policy-generating behaviour is not totally greedy.\n",
        "$$\n",
        "y_{i}^{Sarsa}=R_{t+1}+γQ(S_{t+1},A_{t+1};θ_i)\n",
        "$$\n",
        "$$\n",
        "y_{i}^{Q-learning}=R_{t+1}+γ\\max_{a}Q(S_{t+1},a;θ_i)\n",
        "$$\n",
        "\n",
        "In our NFQ implemenation, we will use the off-policy TD target. Substituting $y_i^{Q-learning}$ into the above loss function $L_i$:\n",
        "$$L_i(θ_i)=\\mathbb{E}_{s,a,r,s'}\\left[\\left(r+γ\\max_{a'}Q(s',a';θ_i)-Q(s,a;\\theta_i)\\right)^2\\right]\n",
        "$$\n",
        "\n",
        "Notice that when differentiating througth this equation, the gradiet does not involve the target y, the gradient must only go through the predicted value Q:\n",
        "$$\n",
        "∇_{θ_i} L_i(θ_i)=\\mathbb{E}_{s,a,r,s'}\\left[\\left(r+γ\\max_{a'}Q(s',a';θ_i)-Q(s,a;\\theta_i)\\right)\\nabla_{θ_i}Q(s,a;\\theta_i)\\right]\n",
        "$$"
      ],
      "metadata": {
        "id": "j8824sMizI96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NFQ():\n",
        "  def __init__(self,\n",
        "               value_model_fn,\n",
        "               value_optimizer_fn,\n",
        "               value_optimizer_lr,\n",
        "               training_strategy_fn,\n",
        "               evaluation_strategy_fn,\n",
        "               batch_size,\n",
        "               epochs):\n",
        "    self.value_model_fn = value_model_fn\n",
        "    self.value_optimizer_fn = value_optimizer_fn\n",
        "    self.value_optimizer_lr = value_optimizer_lr\n",
        "    self.training_strategy_fn = training_strategy_fn\n",
        "    self.evaluation_strategy_fn = evaluation_strategy_fn\n",
        "    self.batch_size = batch_size\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def optimize_model(self, experiences):\n",
        "    states, actions, rewards, next_states, is_terminals = experiences\n",
        "    batch_size = len(is_terminals)\n",
        "\n",
        "    # we get the values of Q-function at s_prime (next_state).\n",
        "    # 'next_states' is plural because it's a batch of 'next_state'.\n",
        "    # detach() here becasue we don't propagate values through this, this keeps\n",
        "    # the predicted vlaue a constant since we only use it to calculate targets.\n",
        "    q_sa = self.online_model(next_states).detach()\n",
        "    max_a_q_sp = q_sa.max(1)[0].unsqueeze(1)\n",
        "    # *(1-is_terminals) to ensure terminal states are grounded to zero.\n",
        "    target_q_s = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
        "\n",
        "    # get the current estimate of Q(s,a).\n",
        "    q_sa = self.online_model(states).gather(1, actions)\n",
        "\n",
        "    td_errors = target_q_s - q_sa\n",
        "    value_loss = td_errors.pow(2).mul(0.5).mean()\n",
        "    # back prop\n",
        "    self.value_optimizer.zero_grad()\n",
        "    value_loss.backward()\n",
        "    self.value_optimizer.step()\n",
        "\n",
        "  def interaction_step(self, state, env):\n",
        "    action = self.training_strategy.select_action(self.online_model, state)\n",
        "    new_state, reward, is_terminal, info = env.step(action)\n",
        "    # the cart-pole env has a wrapper that artificially teminates an episode after\n",
        "    # some time steps. So we add extra code to check this.\n",
        "    is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
        "    is_failure = is_terminal and not is_truncated\n",
        "    experience = (state, action, reward, new_state, float(is_failure))\n",
        "    # print(state)\n",
        "    # print(action)\n",
        "    # print(reward)\n",
        "    # print(new_state)\n",
        "    # print(experience) ###############\n",
        "    # print('============')\n",
        "\n",
        "    self.experiences.append(experience)\n",
        "    self.episode_reward[-1] += reward\n",
        "    self.episode_timestep[-1] += 1\n",
        "    self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
        "    return new_state, is_terminal\n",
        "\n",
        "  def train(self, make_env_fn, make_env_kargs,\n",
        "            seed, gamma,\n",
        "            max_minutes, max_episodes, goal_mean_100_reward):\n",
        "    training_start, last_debug_time = time.time(), float('-inf')\n",
        "\n",
        "    self.checkpoint_dir = tempfile.mkdtemp()\n",
        "    self.make_env_fn = make_env_fn\n",
        "    self.make_env_kargs = make_env_kargs\n",
        "    self.seed = seed\n",
        "    self.gamma = gamma\n",
        "\n",
        "    env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
        "    torch.manual_seed(self.seed); np.random.seed(self.seed); random.seed(self.seed)\n",
        "\n",
        "    nS, nA = env.observation_space.shape[0], env.action_space.n\n",
        "    self.episode_timestep = []\n",
        "    self.episode_reward = []\n",
        "    self.episode_seconds = []\n",
        "    self.evaluation_scores = []\n",
        "    self.episode_exploration = []\n",
        "\n",
        "    self.online_model = self.value_model_fn(nS, nA)\n",
        "    self.value_optimizer = self.value_optimizer_fn(self.online_model,\n",
        "                                                   self.value_optimizer_lr)\n",
        "\n",
        "    self.training_strategy = training_strategy_fn()\n",
        "    self.evaluation_strategy = evaluation_strategy_fn()\n",
        "    self.experiences = []\n",
        "\n",
        "    # the 5 dim are: total_step, mean_100_reward, mean_100_eval_score, training_time, wallclock_elapsed\n",
        "    result = np.empty((max_episodes, 5))\n",
        "    result[:] = np.nan\n",
        "    training_time = 0\n",
        "    for episode in range(1, max_episodes+1):\n",
        "      episode_start = time.time()\n",
        "      state, is_terminal = env.reset(), False\n",
        "      self.episode_reward.append(0.0)\n",
        "      self.episode_timestep.append(0.0)\n",
        "      self.episode_exploration.append(0.0)\n",
        "\n",
        "      for step in count(): # starting from 0\n",
        "        state, is_treminal = self.interaction_step(state, env)\n",
        "        # self.experiences is appended by self.interaction_step()\n",
        "\n",
        "        if len(self.experiences) >= self.batch_size:\n",
        "          experiences = np.array(self.experiences, dtype=object)\n",
        "          batches = [np.vstack(sars) for sars in experiences.T]\n",
        "          experiences = self.online_model.load(batches)\n",
        "          for _ in range(self.epochs):\n",
        "            self.optimize_model(experiences)\n",
        "          self.experiences.clear()\n",
        "\n",
        "        if is_terminal:\n",
        "          gc.collect()\n",
        "          break\n",
        "\n",
        "      # stats\n",
        "      episode_elapsed = time.time() - episode_start\n",
        "      self.episode_seconds.append(episode_elapsed)\n",
        "      training_time += episode_elapsed\n",
        "      evaluation_score, _ = self.evaluate(self.online_model, env)\n",
        "      self.save_checkpoint(episode-1, self.online_model)\n",
        "\n",
        "      total_step = int(np.sum(self.episode_timestep))\n",
        "      self.evaluation_scores.append(evaluation_score)\n",
        "\n",
        "      mean_10_reward = np.mean(self.episode_reward[-10:])\n",
        "      std_10_reward = np.std(self.episode_reward[-10:])\n",
        "\n",
        "      mean_100_reward = np.mean(self.episode_reward[-100:])\n",
        "      std_100_reward = np.std(self.episode_reward[-100:])\n",
        "\n",
        "      mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
        "      std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
        "\n",
        "      lst_100_exp_rat = np.array(self.episode_exploration[-100:]) \\\n",
        "        / np.array(self.episode_timestep[-100:])\n",
        "      mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
        "      std_100_exp_rat = np.std(lst_100_exp_rat)\n",
        "\n",
        "      wallclock_elapsed = time.time() - training_start\n",
        "      result[episode-1] = total_step, mean_100_reward, \\\n",
        "        mean_100_eval_score, training_time, wallclock_elapsed\n",
        "\n",
        "      reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
        "      reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
        "      reached_max_episodes = episode >= max_episodes\n",
        "      reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
        "      training_is_over = reached_max_minutes or reached_max_episodes \\\n",
        "        or reached_goal_mean_reward\n",
        "\n",
        "      elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-training_start))\n",
        "      debug_message = 'el {}, ep {:04}, ts {:06}, ' \\\n",
        "        'ar 10 {:05.1f}\\u00B1{:05.1f}, ' \\\n",
        "        '100 {:05.1f}\\u00B1{:05.1f}, ' \\\n",
        "        'ex 100 {:02.1f}\\u00B1{:02.1f}, ' \\\n",
        "        'ev {:05.1f}\\u00B1{:05.1f}'\n",
        "      debug_message = debug_message.format(\n",
        "        elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward,\n",
        "        mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
        "        mean_100_eval_score, std_100_eval_score\n",
        "      )\n",
        "      print(debug_message, end='\\r', flush=True)\n",
        "      if reached_debug_time or training_is_over:\n",
        "        print(ERASE_LINE + debug_message, flush=True)\n",
        "        last_debug_time = time.time()\n",
        "      if training_is_over: # print crosses and ticks\n",
        "        if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
        "        if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
        "        if reached_goal_mean_reward: print(u'--> reach_goal_mean_reward \\u2713')\n",
        "        break # break out of for step in count() loop\n",
        "\n",
        "    final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n",
        "    wallclock_time = time.time() - training_start\n",
        "    print('Training complete.')\n",
        "    print(f'Final evaluation score {final_eval_score:.2f}\\u00B1{score_std:.2f}'\\\n",
        "          ' in {training_time:.2f}s training time, {wallclock_time:.2f}s wallclock time.\\n')\n",
        "    env.close(); del env\n",
        "    self.get_cleaned_checkpoints()\n",
        "    return result, final_eval_score, training_time, wallclock_time\n",
        "\n",
        "  def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
        "    rs = [] # rewards\n",
        "    for _ in range(n_episodes):\n",
        "      # state, is_terminal\n",
        "      s, d = eval_env.reset(), False\n",
        "      rs.append(0)\n",
        "      for _ in count():\n",
        "        a = self.evaluation_strategy.select_action(eval_policy_model, s)\n",
        "        s, r, d, _ = eval_env.step(a)\n",
        "        rs[-1] += r\n",
        "        if d: break\n",
        "    return np.mean(rs), np.std(rs)\n",
        "\n",
        "  def get_cleaned_checkpoints(self, n_checkpoints=5):\n",
        "    try:\n",
        "      return self.checkpoint_paths\n",
        "    except AttributeError:\n",
        "      self.checkpint_paths={}\n",
        "\n",
        "    paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
        "    # the file name is like 'dir/model.0.tar', 'dir/model.1.tar', etc.\n",
        "    paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
        "    last_ep = max(paths_dic.keys())\n",
        "    checkpoint_idxs = np.linespace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int16)-1\n",
        "\n",
        "    for idx, path in paths_dic.items():\n",
        "      if idx in checkpoint_idxs:\n",
        "        self.checkpoint_paths[idx] = path\n",
        "      else:\n",
        "        os.unlink(path)\n",
        "\n",
        "    return self.ceckpoint_paths\n",
        "\n",
        "  def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n",
        "    env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
        "\n",
        "    checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "    last_ep = max(checkpoint_paths.keys())\n",
        "    self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
        "\n",
        "    self.evaluate(self.online_model, env, n_episodes=n_episodes)\n",
        "    env.close()\n",
        "    data = get_gif_html(env_videos=env.videos,\n",
        "                        title=title.format(self.__class__.__name__),\n",
        "                        max_n_videws=max_n_videos)\n",
        "    del env\n",
        "    return HTML(data=data)\n",
        "\n",
        "  def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n",
        "    env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
        "\n",
        "    checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "    for i in sorted(checkpoint_paths.keys()):\n",
        "      self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
        "      self.evaluate(self.online_model, env, n_episodes=1)\n",
        "\n",
        "    env.close()\n",
        "    data = get_gif_html(env_videos=env.videos,\n",
        "                        title=title.format(self.__class__.__name__),\n",
        "                        subtitle_eps=sorted(checkpoint_paths.keys()),\n",
        "                        max_n_videos=max_n_videos)\n",
        "    del env\n",
        "    return HTML(data=data)\n",
        "\n",
        "  def save_checkpoint(self, episode_idx, model):\n",
        "    torch.save(model.state_dict(),\n",
        "               os.path.join(self.checkpoint_dir, f'model.{episode_idx}.tar'))"
      ],
      "metadata": {
        "id": "nY5xgNv6WxY8"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## select an Optimization method\n",
        "Gradient descent is a stable optimisation method given the following assumptions:\n",
        "1. IID assumption - Idependent and Identically Distributed.\n",
        "2. targets are stationary.\n",
        "\n",
        "However, in RL, the above assumptions don't hold. In practice, both Adam (Adaptive movement estimation) and RMSprop are sensible choices for value-based DRL methods.\n",
        "\n",
        "NFQ has 3 main steps:\n",
        "1. collect E experiences: (s, a, r, s', d) tuples. We use 1024 samples. This is batch_size in code\n",
        "2. calculate the off-policy TD targets: $r+γ\\max_{a'}Q(s',a';θ)$\n",
        "3. fit the action-value function Q(s,a;θ): using MSE and RMSprop\n",
        "\n",
        "The algo repeats steps 2 and 3 $K$ number of times before going back to step 1. That's what makes it 'fitted'; the nested loop. We will use 40 fitting steps $K$. This is epoch in code."
      ],
      "metadata": {
        "id": "H6RcSqbF_mE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nfq_results = []\n",
        "best_agent, best_eval_score = None, float('-inf')\n",
        "for seed in SEEDS:\n",
        "  environment_settings = {\n",
        "      'env_name': 'CartPole-v1',\n",
        "      'gamma': 1.00,\n",
        "      'max_minutes': 20,\n",
        "      'max_episodes': 10000,\n",
        "      'goal_mean_100_reward': 475\n",
        "  }\n",
        "\n",
        "  value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512, 128))\n",
        "  value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
        "  value_optimizer_lr = 5e-4\n",
        "\n",
        "  training_strategy_fn = lambda: EGreedyStrategy(epsilon=0.5) # or 0.05\n",
        "  evaluation_strategy_fn = lambda: GreedyStrategy()\n",
        "\n",
        "  batch_size = 1024\n",
        "  epochs = 40\n",
        "\n",
        "  env_name, gamma, max_minutes, max_episodes, goal_mean_100_reward = environment_settings.values()\n",
        "  agent = NFQ(value_model_fn,\n",
        "              value_optimizer_fn,\n",
        "              value_optimizer_lr,\n",
        "              training_strategy_fn,\n",
        "              evaluation_strategy_fn,\n",
        "              batch_size,\n",
        "              epochs)\n",
        "\n",
        "  make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
        "  result, final_eval_score, training_time, wallcock_time = agent.train(\n",
        "      make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward\n",
        "  )\n",
        "  nfq_results.append(result)\n",
        "  if final_eval_score > best_eval_score:\n",
        "    best_eval_score = final_eval_score\n",
        "    best_agent = agent\n",
        "\n",
        "nfq_results = np.array(nfq_results)\n",
        "_ = BEEP()\n"
      ],
      "metadata": {
        "id": "11m6JbPJ8iSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}