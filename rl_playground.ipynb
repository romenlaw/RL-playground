{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romenlaw/RL-playground/blob/main/rl_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TXRzOQR4kYX",
        "outputId": "fc3546e5-a197-425a-cfb4-5f2def6fd3f1"
      },
      "id": "8TXRzOQR4kYX",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-walk\n",
            "  Cloning https://github.com/mimoralea/gym-walk to /tmp/pip-install-r7vykhjy/gym-walk_acf1c126f853451ca4ddad83e51812b7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mimoralea/gym-walk /tmp/pip-install-r7vykhjy/gym-walk_acf1c126f853451ca4ddad83e51812b7\n",
            "  Resolved https://github.com/mimoralea/gym-walk to commit 5999016267d6de2f5a63307fb00dfd63de319ac1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-walk) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-walk) (0.0.8)\n",
            "Building wheels for collected packages: gym-walk\n",
            "  Building wheel for gym-walk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-walk: filename=gym_walk-0.0.2-py3-none-any.whl size=4058 sha256=57478353b7fd4a05a3e9e36f7a1701fa18ad958dda05aa8f2ad93ebfe65b77a1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4afxq882/wheels/24/fe/c4/0cbc7511d29265bad7e28a09311db3f87f0cafba74af54d530\n",
            "Successfully built gym-walk\n",
            "Installing collected packages: gym-walk\n",
            "Successfully installed gym-walk-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym, gym_walk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPaUdewi3ncY",
        "outputId": "0547ea09-8c33-401a-990a-40a0bc336be6"
      },
      "id": "lPaUdewi3ncY",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MDP\n",
        "Markov Decision Process:\n",
        "* Partially-Observable Markov Decision Process (POMDP): When the agent cannot fully observe the environment state.\n",
        "* Factored Markov Decision Process (FMDP): Allows the representation of the transition and reward function more compactly so that we can represent very large MDPs.\n",
        "* Continuous [Time|Action|State] Markov Decision Process: When either time, action, state or any combination of them are continuous.\n",
        "* Relational Markov Decision Process (RMDP): Allows the combination of probabilistic and relational knowledge.\n",
        "* Semi-Markov Decision Process (SMDP): Allows the inclusion of abstract actions that can take multiple time steps to complete.\n",
        "* Multi-Agent Markov Decision Process (MMDP): Allows the inclusion of multiple agents in the same environment.\n",
        "* Decentralized Markov Decision Process (Dec-MDP): Allows for multiple agents to collaborate and maximize a common reward.\n",
        "\n",
        "$MDP(\\mathcal S,\\mathcal A,\\mathcal T,\\mathcal R, \\mathcal S_{\\theta}, \\gamma, \\mathcal H)$\n",
        "\n",
        "$POMDP(\\mathcal S,\\mathcal A,\\mathcal T,\\mathcal R, \\mathcal S_{\\theta}, \\gamma, \\mathcal H, \\mathcal O, \\mathcal E)$\n",
        "\n",
        "$\\mathcal S$ is state space,\n",
        "$\\mathcal A$ is action space,\n",
        "$\\mathcal T$ is transition function,\n",
        "$\\mathcal R$ is reward signal,\n",
        "$\\mathcal S_{\\theta}$ is initial states distribution,\n",
        "$\\gamma$ is discount factor,\n",
        "$\\mathcal H$ is horizon,\n",
        "$\\mathcal O$ is observation space,\n",
        "$\\mathcal E$ is emission probability that defines the prob of showing an observation $o_t$ at given state $s_t$.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Tk0Fht_8BEY"
      },
      "id": "6Tk0Fht_8BEY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "* The outer dict keys are States.\n",
        "* The inner dict keys are Actions.\n",
        "\n",
        "{0: {0: [(0.8, 0, 0.0, True), ...], ...\n",
        "\n",
        "In State 0, Action 0 has 80% chance to transit into State 0, which has Reward 0.0 and is a terminal state..."
      ],
      "metadata": {
        "id": "TH17Y7MJ5p2W"
      },
      "id": "TH17Y7MJ5p2W"
    },
    {
      "cell_type": "code",
      "source": [
        "P = gym.make('BanditSlipperyWalk-v0').env.P\n",
        "P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfRXIV4K3vTG",
        "outputId": "be3e28b4-6088-49dc-baef-25c334711ac0"
      },
      "id": "hfRXIV4K3vTG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)],\n",
              "  1: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)]},\n",
              " 1: {0: [(0.8, 0, 0.0, True), (0.0, 1, 0.0, False), (0.2, 2, 1.0, True)],\n",
              "  1: [(0.8, 2, 1.0, True), (0.0, 1, 0.0, False), (0.2, 0, 0.0, True)]},\n",
              " 2: {0: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)],\n",
              "  1: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P = gym.make('BanditWalk-v0').env.P\n",
        "P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlwp6NWa63-n",
        "outputId": "b9328f22-260c-470d-dfcb-fec596d48b91"
      },
      "id": "Vlwp6NWa63-n",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)],\n",
              "  1: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)]},\n",
              " 1: {0: [(1.0, 0, 0.0, True), (0.0, 1, 0.0, False), (0.0, 2, 1.0, True)],\n",
              "  1: [(1.0, 2, 1.0, True), (0.0, 1, 0.0, False), (0.0, 0, 0.0, True)]},\n",
              " 2: {0: [(1.0, 2, 0.0, True), (0.0, 2, 0.0, True), (0.0, 2, 0.0, True)],\n",
              "  1: [(1.0, 2, 0.0, True), (0.0, 2, 0.0, True), (0.0, 2, 0.0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P = gym.make('FrozenLake-v1').env.P\n",
        "P"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqbria-B7LMD",
        "outputId": "478c3d7d-4be8-48e3-f361-fe09de0aa88d"
      },
      "id": "Zqbria-B7LMD",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False)]},\n",
              " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True)],\n",
              "  1: [(0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 2, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 0, 0.0, False)]},\n",
              " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 6, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 1, 0.0, False),\n",
              "   (0.3333333333333333, 6, 0.0, False),\n",
              "   (0.3333333333333333, 3, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 6, 0.0, False),\n",
              "   (0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 2, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 1, 0.0, False)]},\n",
              " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 7, 0.0, True)],\n",
              "  1: [(0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 7, 0.0, True),\n",
              "   (0.3333333333333333, 3, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 7, 0.0, True),\n",
              "   (0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 3, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 3, 0.0, False),\n",
              "   (0.3333333333333333, 2, 0.0, False)]},\n",
              " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 8, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True)],\n",
              "  2: [(0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 0, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 0, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 10, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 7, 0.0, True)],\n",
              "  2: [(0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 7, 0.0, True),\n",
              "   (0.3333333333333333, 2, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 7, 0.0, True),\n",
              "   (0.3333333333333333, 2, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 12, 0.0, True)],\n",
              "  1: [(0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 12, 0.0, True),\n",
              "   (0.3333333333333333, 9, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 12, 0.0, True),\n",
              "   (0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 4, 0.0, False),\n",
              "   (0.3333333333333333, 8, 0.0, False)]},\n",
              " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 13, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 8, 0.0, False),\n",
              "   (0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 10, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True)],\n",
              "  3: [(0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 5, 0.0, True),\n",
              "   (0.3333333333333333, 8, 0.0, False)]},\n",
              " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
              "   (0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 11, 0.0, True)],\n",
              "  2: [(0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 11, 0.0, True),\n",
              "   (0.3333333333333333, 6, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 11, 0.0, True),\n",
              "   (0.3333333333333333, 6, 0.0, False),\n",
              "   (0.3333333333333333, 9, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 12, 0.0, True),\n",
              "   (0.3333333333333333, 13, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 12, 0.0, True),\n",
              "   (0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False)],\n",
              "  2: [(0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 9, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 9, 0.0, False),\n",
              "   (0.3333333333333333, 12, 0.0, True)]},\n",
              " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False)],\n",
              "  1: [(0.3333333333333333, 13, 0.0, False),\n",
              "   (0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 15, 1.0, True)],\n",
              "  2: [(0.3333333333333333, 14, 0.0, False),\n",
              "   (0.3333333333333333, 15, 1.0, True),\n",
              "   (0.3333333333333333, 10, 0.0, False)],\n",
              "  3: [(0.3333333333333333, 15, 1.0, True),\n",
              "   (0.3333333333333333, 10, 0.0, False),\n",
              "   (0.3333333333333333, 13, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Return\n",
        "The Return is the sum of Rewards encountered from step t until the final step T:\n",
        "$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + ...+ R_T$$\n",
        "\n",
        "With discount factor:\n",
        "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + ...+ \\gamma^{T-1}R_T$$\n",
        "$$G_t = \\sum_{k=0}^\\infty \\gamma^kR_{t+1+k}$$\n",
        "\n",
        "In recursive form:\n",
        "$$G_t = R_{t+1} + \\gamma G_{t+1}$$"
      ],
      "metadata": {
        "id": "JZDDuPcQ__Qq"
      },
      "id": "JZDDuPcQ__Qq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State-Value Function V\n",
        "AKA Value Function, V-function, $V^\\pi(s)$...\n",
        "\n",
        "Value of state s when following a policy π:\n",
        "$$v_\\pi(s)=\\mathbb{E}_\\pi[G_t|S_t=s]$$\n",
        "Value of a State s under Policy \\pi is the Expectation over \\pi of Returns at time step t given you select State s at time step t.\n",
        "Use recursive form of Return:\n",
        "$$v_\\pi(s)=\\mathbb{E}_\\pi[R_t+\\gamma G_{t+1}|S_t=s]$$\n",
        "\n",
        "**Bellman equation**: it tells us how to find the value of states\n",
        "$$v_\\pi(s)=\\sum_a \\pi(a|s)\\sum_{s', r}\\{p(s',r|s,a)[r+\\gamma v_\\pi(s')]\\}, \\forall s \\in S$$\n",
        "\n",
        "* We get the action (or actions, if policy is stocastic) prescribed for state s. And do a weighted sum...\n",
        "* We also weight the sum over the probability of next states and rewards.\n",
        "* We add the reward and the discounted value of the landing states, weight that by the probabilities.\n",
        "* do this for all states in the state space\n",
        "\n"
      ],
      "metadata": {
        "id": "Dzu6DUvKFXOD"
      },
      "id": "Dzu6DUvKFXOD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action-Value Function Q\n",
        "What to expect from here if I do this?\n",
        "\n",
        "AKA Q-function. or $Q^\\pi(s,a)$: gives us the expected Return if agent follows policy π after taking action a in state s.\n",
        "\n",
        "$$q_\\pi(s, a)=\\mathbb{E}_\\pi[G_t|S_t=s, A_t=a]$$\n",
        "The value of Action a in State s is the expectation of Returns given we select action a in state s and follow policy π thereafter.\n",
        "\n",
        "In recursive form:\n",
        "$$q_\\pi(s,a) =  \\mathbb{E}_\\pi[R_t+\\gamma G_{t+1}|S_t=s, A_t=a]$$\n",
        "\n",
        "The Bellman equation for action-values:\n",
        "$$q_\\pi(s,a)=\\sum_{s',r}\\{p(s',r|s,a)[r+\\gamma v_\\pi(s')]\\}, \\forall s \\in S, \\forall a \\in A(s)$$\n"
      ],
      "metadata": {
        "id": "KTRTzoQ1K34A"
      },
      "id": "KTRTzoQ1K34A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action-Advantage Function\n",
        "i.e. how much better if I do this (comparing to taking the default action under policy π?\n",
        "\n",
        "The advantage of action a in state s under a policy π:\n",
        "$$a_\\pi(s,a)=q_\\pi(s,a)-v_\\pi(s)$$\n"
      ],
      "metadata": {
        "id": "NZ9O6sqWPiFz"
      },
      "id": "NZ9O6sqWPiFz"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMVvKZDq_-hL"
      },
      "id": "fMVvKZDq_-hL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}