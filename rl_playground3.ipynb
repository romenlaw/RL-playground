{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romenlaw/RL-playground/blob/main/rl_playground3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "CXH_dA7uqL9o"
      },
      "id": "CXH_dA7uqL9o"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk &>/dev/null\n",
        "!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima &>/dev/null"
      ],
      "metadata": {
        "id": "E26jsafbqPJA"
      },
      "id": "E26jsafbqPJA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_walk, gym_aima\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from itertools import cycle, count\n",
        "import itertools\n",
        "from tabulate import tabulate\n",
        "\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "SEEDS = (12, 34, 56, 78, 90)\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hDfePrOBqVOf"
      },
      "id": "hDfePrOBqVOf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "ogaFETxXhCGO"
      },
      "id": "ogaFETxXhCGO"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"exponentially decaying schedule\n",
        "this function allows you to calculate all the values for alpha for the full training process\n",
        "\"\"\"\n",
        "def decay_schedule(init_value, min_value,\n",
        "                   decay_ratio, # determines how many episodes to use for decay\n",
        "                   max_steps, # i.e. n_episodes in previous chapters\n",
        "                   log_start=-2, log_base=10):\n",
        "  assert min_value<=init_value, \"min_value must be <= init_value\"\n",
        "  decay_steps = max(int(max_steps*decay_ratio), 1)\n",
        "  rem_steps = max_steps - decay_steps # remaining steps (i.e. not used for decay)\n",
        "\n",
        "  # calculate actual values of an inverse log curve ([::-1] reverse the order)\n",
        "  values = np.logspace(start=log_start, stop=0,\n",
        "                       num=decay_steps, # number of samples to generate\n",
        "                       base=log_base,\n",
        "                       endpoint=True # samples are inclusive of 'stop'\n",
        "                       )[::-1]\n",
        "  # print(\"reverse logspace: \", values)\n",
        "  # normalise to between 0 and 1\n",
        "  values = (values - values.min()) / (values.max() - values.min())\n",
        "  # transform the points to lay between init_value and min_value\n",
        "  values = min_value + (init_value - min_value) * values\n",
        "  values = np.pad(values, (0, rem_steps), 'edge')\n",
        "  return values\n",
        "\n",
        "\"\"\"Generate full trajectory\n",
        "Running a policy and extracting the collection of experience tuples\n",
        "(the trajectories) for offline processing.\n",
        "\"\"\"\n",
        "def generate_trajectory(pi, env, max_steps=200):\n",
        "  done, trajectory =  False, []\n",
        "  while not done:\n",
        "    state = env.reset()\n",
        "    trajectory = []\n",
        "    t = 0\n",
        "    while t<max_steps and not done: # max_steps allows truncation of long trajectory\n",
        "      action = pi(state)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      experience = (state, action, reward, next_state, done)\n",
        "      trajectory.append(experience)\n",
        "      state = next_state\n",
        "\n",
        "  return np.array(trajectory, np.object_)"
      ],
      "metadata": {
        "id": "2O9Xa0zHhDre"
      },
      "id": "2O9Xa0zHhDre",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the replace-trace strategy, traces are set to 1 when a state-action pair is visited, and decay based on \\lambda value just like accumulate-trace strategy (in chapter 5).\n",
        "\n",
        "Diff between replace-trace and accumulate-trace:\n",
        "* accumulate-trace tracks the visited states; the eligibility trace is without bound\n",
        "* replace-trace tracks the visited state-action pair; also the eligibility trace is clipped to 1 to avoid dead loop"
      ],
      "metadata": {
        "id": "MdXxluwOc1KH"
      },
      "id": "MdXxluwOc1KH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sarsa(\\lambda)\n",
        "\n",
        "This is a mix of Sarsa and TD(\\lambda) methods."
      ],
      "metadata": {
        "id": "RNtJEXXRe2-i"
      },
      "id": "RNtJEXXRe2-i"
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_lambda(env, gamma=1.0,\n",
        "                 init_alpha=0.5, min_alpha=0.01, alpha_decay_ratio=0.5,\n",
        "                 init_epsilon=1.0, min_epsilon=0.1, epsilon_decay_ratio=0.9,\n",
        "                 lambda_=0.5, # lambda is reserved word in python, so add _\n",
        "                 replacing_traces=True,\n",
        "                 n_episodes=3000):\n",
        "  nS = env.observation_sapece.n\n",
        "  nA = env.action_space.n\n",
        "  pi_track = []\n",
        "  Q = np.zeros((nS, nA), dtype=np.float32)\n",
        "  Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float32)\n",
        "  # eligibility trace keeps track of state-action pairs\n",
        "  E = np.zeros((nS, nA), dtype=np.float32)\n",
        "  select_action = lambda state, Q, epsilon: \\\n",
        "    np.argmax(Q[state]) if np.random.random() > epsilon \\\n",
        "    else np.random.randint(len(Q[state])) # random action of the state\n",
        "\n",
        "  alphas = decay_schedule(init_alpha, min_alpha, alphs_decay_ratio, n_episodes)\n",
        "  epsilons = decay_schedule(init_epsilon, min_epsilon, epsilon_decay_ratio, n_episodes)\n",
        "\n",
        "  for e in tqdm(range(n_episodes), leave=False):\n",
        "    E.fill(0) # at every episode, we reset eligitability of every state-action to 0\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    action = select_action(state, Q, epsilons[e])\n",
        "\n",
        "    while not done:\n",
        "      # collect experience\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      next_action = select_action(next_state, Q, epsilons[e])\n",
        "\n",
        "      # same as original sarsa\n",
        "      td_target = reward + gamma * Q[next_state][next_action] * (not done)\n",
        "      td_error = td_target - Q[state][action]\n",
        "\n",
        "      # increment state-action pair trace, clip to 1 if it's replacing trace\n",
        "      E[state][action]+=1\n",
        "      if replacing_traces: E.clip(0,1, out=E)\n",
        "\n",
        "      # notice we update entire Q table for all eligible state-action pairs\n",
        "      Q = Q + alphas[e] * td_error * E\n",
        "      E = gamma * lambda_ * E # decay E\n",
        "\n",
        "      state, action = next_state, next_action\n",
        "\n",
        "    Q_track[e] = Q\n",
        "    pi_track.append(np.argmax(Q[state], axis=1)) # axis 1 is action\n",
        "\n",
        "  V = np.max(Q, axis=1)\n",
        "  pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
        "\n",
        "  return Q, V, pi, Q_track, pi_track"
      ],
      "metadata": {
        "id": "uhfJ7WAdfF_b"
      },
      "id": "uhfJ7WAdfF_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Watkin's Q(\\lambda)\n",
        "\n",
        "it's an off-policy control version of \\lambda algorithms. Q(\\lambda) is Q-learning using \\lambda return for policy evaluation of the GPI pattern."
      ],
      "metadata": {
        "id": "hOSbj-wP_S_o"
      },
      "id": "hOSbj-wP_S_o"
    },
    {
      "cell_type": "code",
      "source": [
        "def q_lambda(env, gamma=1.0,\n",
        "             init_alpha=0.5, min_alpha=0.01, alpha_decay_ratio=0.5,\n",
        "             init_epsilon=1.0, min_epsilon=0.1, epsilon_decay_ratio=0.9,\n",
        "             lambda_=0.5,\n",
        "             replacing_traces=True,\n",
        "             n_episodes=3000):\n",
        "  nS = env.observation_space.n\n",
        "  nA = env.action_space.n\n",
        "  pi_track=[]\n",
        "  Q = np.zeros((nS, nA), dtype=np.float32)\n",
        "  Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float32)\n",
        "  E = np.zeros((nS, nA), dtype=np.float32)\n",
        "  select_action = lambda state, Q, epsilon: \\\n",
        "    np.argmax(Q[state]) if np.random.random() > epsilon \\\n",
        "    else np.random.randint(Q[state])\n",
        "  alphas = decay_schedule(init_alpha, min_alpha, alpha_decay_ratio, n_episodes)\n",
        "  epsilons = decay_schedule(init_epsilon, min_epsilon, epsilonn_decay_ratio, n_episodes)\n",
        "\n",
        "  for e in tqdm(range(n_episodes), leave=False):\n",
        "    E.fill(0)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    action = select_action(state, Q, epsilons[e])\n",
        "    while not done:\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      next_action = select_action(next_state, Q, epsilons[e])\n",
        "      # verify that action on next step is still from greedy policy\n",
        "      next_action_is_greedy = Q[next_state][next_action] == Q[next_state].max()\n",
        "      td_target = reward + gamma * Q[next_state].max() * (not done)\n",
        "      td_error = td_target - Q[state][action] # note: use current state\n",
        "\n",
        "      if replacing_traces: E[state].fill(0)\n",
        "\n",
        "      E[state][action] += 1\n",
        "      Q = Q + alphas[e] * td_error + E\n",
        "\n",
        "      if next_action_is_greedy:\n",
        "        E *= gamma * lambda_ # decay E as usual\n",
        "      else:\n",
        "        E.fill(0) # reset E because we want to learn greedy policy\n",
        "\n",
        "      state, action = next_state, next_action\n",
        "\n",
        "    Q_track[e] = Q\n",
        "    pi_track.append(np.max(Q, axis=1))\n",
        "\n",
        "  V = np.max(Q, axis=1)\n",
        "  pi = lambda s: {s:a for s,a in enumerate(np.argmax(Q, axis=1))}[s]\n",
        "  return Q"
      ],
      "metadata": {
        "id": "rfksOHiBA0mq"
      },
      "id": "rfksOHiBA0mq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dyna-Q\n",
        "Unifying model-free and model-based/planning methods  by interleaving a model-free method (Q-learning) and a planning method (similar to Value Iteration)."
      ],
      "metadata": {
        "id": "SI-mQjIjEATy"
      },
      "id": "SI-mQjIjEATy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}